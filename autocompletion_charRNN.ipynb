{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autosuggestion - Predicting the next character using RNN\n",
    "\n",
    "Adapted from: https://medium.com/towards-artificial-intelligence/sentence-prediction-using-word-level-lstm-text-generator-language-modeling-using-rnn-a80c4cda5b40\n",
    "\n",
    "\n",
    "### Approach\n",
    "\n",
    "- Test with bigger dataset (17000 sentences) vs smaller dataset (1700 sentences)\n",
    "    - Goal: to identify if a larger training set improves prediction accuracy\n",
    "    - Bigger dataset: sample of Enron email vs smaller dataset: personal Acronis email made up of mostly newsletters\n",
    "    \n",
    "### Steps\n",
    "\n",
    "1. Get sentence tokens and clean sentences\n",
    "2. Remove low frequency words and generate training sequences (10 characters)\n",
    "3. Train model and predict\n",
    "\n",
    "### Setup\n",
    "\n",
    "- Python version: Python 3.7.4\n",
    "- Set up virtual environment\n",
    "    - pip install --user virtualenv\n",
    "    - virtualenv tensorflow-gpu\n",
    "    - tensorflow-gpu\\Scripts\\activate\n",
    "- Add Jupyter Notebook to virtual environment\n",
    "    - pip install ipykernel\n",
    "    - python -m ipykernel install --name=tensorflow-gpu\n",
    "- Set up Tensorflow 2.1 GPU version\n",
    "    - https://www.tensorflow.org/install/gpu\n",
    "    - pip install tensorflow==2.1 \n",
    "    - CUDA 10.1, cuDNN 7.6.5\n",
    "- pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"enron_train\", \"r\")\n",
    "if f.mode == 'r':\n",
    "    content = f.read()\n",
    "    # print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Cleaning the data\n",
    "clean_data = []\n",
    "for text in content.splitlines():\n",
    "    text = text.replace('_', ' ')\n",
    "    a = re.sub(r'[^a-zA-z ]+', '', text).strip()\n",
    "    if len(a)>0:\n",
    "        clean_data.append(a)\n",
    "    else:\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the lines which are to short or to long\n",
    "short_data = []\n",
    "for line in clean_data:\n",
    "    if 2 <= len(line.split()) <= 150:\n",
    "        short_data.append(line)\n",
    "    else:\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342066\n"
     ]
    }
   ],
   "source": [
    "# Counting the appearnce of each word in the corpus also calculates the number of unique words also\n",
    "word2count = {}\n",
    "total_words = 0\n",
    "for text in short_data:\n",
    "    for word in text.split():\n",
    "        if word not in word2count:\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] += 1\n",
    "        total_words += 1\n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2526\n"
     ]
    }
   ],
   "source": [
    "# creating a list that will only contain the words that appear more than 15 times\n",
    "word15 = []\n",
    "threshold = 15\n",
    "for word, count in word2count.items():\n",
    "    if count >= threshold:\n",
    "        if len(word) > 1:\n",
    "            word15.append(word)\n",
    "print(len(word15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the words from each string which appear less than 15 times\n",
    "data_15 = []\n",
    "for line in short_data:\n",
    "    str1=''\n",
    "    for word in line.split():\n",
    "        if word in word15:\n",
    "            str1 = \" \".join((str1, word))\n",
    "    data_15.append(str1.lstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the lines which are to short or to long after removing the unnecssary words.     \n",
    "short_data_consize = []\n",
    "for line in data_15:\n",
    "    if 3 <= len(line.split()) <= 150:\n",
    "        short_data_consize.append(line)\n",
    "    else:\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a function to save data\n",
    "def write_txt(name, data):\n",
    "    file1 = open(\"{0}.txt\".format(name),\"w\") \n",
    "    for line in data:\n",
    "        file1.writelines(line) \n",
    "        file1.writelines('\\n') \n",
    "    file1.close() #to change file access modes\n",
    "\n",
    "write_txt(name = 'enron_train_charRNN', data = short_data_consize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filepath):\n",
    "    with open(filepath) as f:\n",
    "        str_text = f.read()\n",
    "    return str_text\n",
    "\n",
    "text = read_file('enron_train_charRNN.txt')\n",
    "line = text.split(\"\\n\")\n",
    "# token_lst = [x.split() for x in line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_lst = line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seq(tokens):\n",
    "    train_len = 9+1\n",
    "    text_sequences = []\n",
    "    for i in range(train_len,len(tokens)):\n",
    "        seq = tokens[i-train_len:i]\n",
    "        text_sequences.append(seq)\n",
    "    return text_sequences\n",
    "\n",
    "seq_lst = [generate_seq(x) for x in token_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = [item for sublist in token_lst for item in sublist]\n",
    "\n",
    "sequences = {}\n",
    "count = 1\n",
    "for i in range(len(flatten)):\n",
    "    if flatten[i] not in sequences:\n",
    "        sequences[flatten[i]] = count\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_seq = [item for sublist in seq_lst for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ting takes',\n",
       " 'ing takes ',\n",
       " 'ng takes t',\n",
       " 'g takes th',\n",
       " ' takes the',\n",
       " 'takes the ',\n",
       " 'akes the f',\n",
       " 'kes the fu',\n",
       " 'es the fun',\n",
       " 's the fun ']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten_seq[40:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sequences = flatten_seq\n",
    "train_len = 9+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(char_level=True, oov_token='UNK')\n",
    "tokenizer.fit_on_texts(text_sequences)\n",
    "sequences = tokenizer.texts_to_sequences(text_sequences) \n",
    "\n",
    "#Collecting some information   \n",
    "vocabulary_size = len(tokenizer.word_counts)\n",
    "\n",
    "import numpy as np\n",
    "n_sequences = np.empty([len(sequences),train_len], dtype='int32')\n",
    "for i in range(len(sequences)):\n",
    "    n_sequences[i] = sequences[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "\n",
    "train_inputs = n_sequences[:,:-1]\n",
    "train_targets = n_sequences[:,-1]\n",
    "\n",
    "train_targets = to_categorical(train_targets, num_classes=vocabulary_size+2)\n",
    "seq_len = train_inputs.shape[1]\n",
    "train_inputs.shape\n",
    "\n",
    "def create_model(vocabulary_size, seq_len):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabulary_size, seq_len,input_length=seq_len))\n",
    "    model.add(LSTM(50,return_sequences=True))\n",
    "    model.add(LSTM(50))\n",
    "    model.add(Dense(50,activation='relu'))\n",
    "    model.add(Dense(vocabulary_size,activation='softmax'))\n",
    "    opt_adam = Adam(lr=0.001)\n",
    "    #You can simply pass 'adam' to optimizer in compile method. Default learning rate 0.001\n",
    "    #But here we are using adam optimzer from optimizer class to change the LR.\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=opt_adam,metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 9, 9)              261       \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 9, 50)             12000     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 29)                1479      \n",
      "=================================================================\n",
      "Total params: 36,490\n",
      "Trainable params: 36,490\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vocabulary_size+2,seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1383145 samples\n",
      "Epoch 1/500\n",
      "1382528/1383145 [============================>.] - ETA: 0s - loss: 1.9869 - accuracy: 0.4091\n",
      "Epoch 00001: loss improved from inf to 1.98667, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 110s 79us/sample - loss: 1.9867 - accuracy: 0.4092\n",
      "Epoch 2/500\n",
      "1382912/1383145 [============================>.] - ETA: 0s - loss: 1.4861 - accuracy: 0.5520\n",
      "Epoch 00002: loss improved from 1.98667 to 1.48603, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 96s 70us/sample - loss: 1.4860 - accuracy: 0.5520\n",
      "Epoch 3/500\n",
      "1382528/1383145 [============================>.] - ETA: 0s - loss: 1.3341 - accuracy: 0.5912\n",
      "Epoch 00003: loss improved from 1.48603 to 1.33409, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 92s 67us/sample - loss: 1.3341 - accuracy: 0.5912\n",
      "Epoch 4/500\n",
      "1382784/1383145 [============================>.] - ETA: 0s - loss: 1.2582 - accuracy: 0.6095\n",
      "Epoch 00004: loss improved from 1.33409 to 1.25818, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 92s 67us/sample - loss: 1.2582 - accuracy: 0.6095\n",
      "Epoch 5/500\n",
      "1382400/1383145 [============================>.] - ETA: 0s - loss: 1.2122 - accuracy: 0.6198\n",
      "Epoch 00005: loss improved from 1.25818 to 1.21224, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 100s 72us/sample - loss: 1.2122 - accuracy: 0.6198\n",
      "Epoch 6/500\n",
      "1382656/1383145 [============================>.] - ETA: 0s - loss: 1.1809 - accuracy: 0.6271\n",
      "Epoch 00006: loss improved from 1.21224 to 1.18091, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 93s 67us/sample - loss: 1.1809 - accuracy: 0.6271\n",
      "Epoch 7/500\n",
      "1382784/1383145 [============================>.] - ETA: 0s - loss: 1.1589 - accuracy: 0.6329\n",
      "Epoch 00007: loss improved from 1.18091 to 1.15886, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 101s 73us/sample - loss: 1.1589 - accuracy: 0.6329\n",
      "Epoch 8/500\n",
      "1383040/1383145 [============================>.] - ETA: 0s - loss: 1.1420 - accuracy: 0.6376\n",
      "Epoch 00008: loss improved from 1.15886 to 1.14199, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 95s 69us/sample - loss: 1.1420 - accuracy: 0.6376\n",
      "Epoch 9/500\n",
      "1382912/1383145 [============================>.] - ETA: 0s - loss: 1.1286 - accuracy: 0.6409\n",
      "Epoch 00009: loss improved from 1.14199 to 1.12858, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 95s 68us/sample - loss: 1.1286 - accuracy: 0.6409\n",
      "Epoch 10/500\n",
      "1383040/1383145 [============================>.] - ETA: 0s - loss: 1.1175 - accuracy: 0.6443\n",
      "Epoch 00010: loss improved from 1.12858 to 1.11753, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 97s 70us/sample - loss: 1.1175 - accuracy: 0.6443\n",
      "Epoch 11/500\n",
      "1382912/1383145 [============================>.] - ETA: 0s - loss: 1.1083 - accuracy: 0.6470\n",
      "Epoch 00011: loss improved from 1.11753 to 1.10829, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 96s 69us/sample - loss: 1.1083 - accuracy: 0.6470\n",
      "Epoch 12/500\n",
      "1382400/1383145 [============================>.] - ETA: 0s - loss: 1.1007 - accuracy: 0.6493\n",
      "Epoch 00012: loss improved from 1.10829 to 1.10070, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 93s 68us/sample - loss: 1.1007 - accuracy: 0.6493\n",
      "Epoch 13/500\n",
      "1382400/1383145 [============================>.] - ETA: 0s - loss: 1.0938 - accuracy: 0.6510\n",
      "Epoch 00013: loss improved from 1.10070 to 1.09377, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 97s 70us/sample - loss: 1.0938 - accuracy: 0.6510\n",
      "Epoch 14/500\n",
      "1382400/1383145 [============================>.] - ETA: 0s - loss: 1.0876 - accuracy: 0.6529\n",
      "Epoch 00014: loss improved from 1.09377 to 1.08763, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 95s 69us/sample - loss: 1.0876 - accuracy: 0.6529\n",
      "Epoch 15/500\n",
      "1382400/1383145 [============================>.] - ETA: 0s - loss: 1.0820 - accuracy: 0.6545\n",
      "Epoch 00015: loss improved from 1.08763 to 1.08197, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 95s 68us/sample - loss: 1.0820 - accuracy: 0.6545\n",
      "Epoch 16/500\n",
      "1382400/1383145 [============================>.] - ETA: 0s - loss: 1.0775 - accuracy: 0.6559\n",
      "Epoch 00016: loss improved from 1.08197 to 1.07748, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 97s 70us/sample - loss: 1.0775 - accuracy: 0.6559\n",
      "Epoch 17/500\n",
      "1382400/1383145 [============================>.] - ETA: 0s - loss: 1.0729 - accuracy: 0.6572\n",
      "Epoch 00017: loss improved from 1.07748 to 1.07286, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 93s 67us/sample - loss: 1.0729 - accuracy: 0.6572\n",
      "Epoch 18/500\n",
      "1382400/1383145 [============================>.] - ETA: 0s - loss: 1.0685 - accuracy: 0.6588\n",
      "Epoch 00018: loss improved from 1.07286 to 1.06856, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 98s 71us/sample - loss: 1.0686 - accuracy: 0.6588\n",
      "Epoch 19/500\n",
      "1382784/1383145 [============================>.] - ETA: 0s - loss: 1.0651 - accuracy: 0.6597\n",
      "Epoch 00019: loss improved from 1.06856 to 1.06515, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 96s 69us/sample - loss: 1.0652 - accuracy: 0.6597\n",
      "Epoch 20/500\n",
      "1382528/1383145 [============================>.] - ETA: 0s - loss: 1.0611 - accuracy: 0.6608\n",
      "Epoch 00020: loss improved from 1.06515 to 1.06117, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 94s 68us/sample - loss: 1.0612 - accuracy: 0.6608\n",
      "Epoch 21/500\n",
      "1382656/1383145 [============================>.] - ETA: 0s - loss: 1.0582 - accuracy: 0.6621\n",
      "Epoch 00021: loss improved from 1.06117 to 1.05818, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 97s 70us/sample - loss: 1.0582 - accuracy: 0.6621\n",
      "Epoch 22/500\n",
      "1382784/1383145 [============================>.] - ETA: 0s - loss: 1.0549 - accuracy: 0.6628\n",
      "Epoch 00022: loss improved from 1.05818 to 1.05491, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 96s 70us/sample - loss: 1.0549 - accuracy: 0.6628\n",
      "Epoch 23/500\n",
      "1382784/1383145 [============================>.] - ETA: 0s - loss: 1.0520 - accuracy: 0.6642\n",
      "Epoch 00023: loss improved from 1.05491 to 1.05202, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 97s 70us/sample - loss: 1.0520 - accuracy: 0.6642\n",
      "Epoch 24/500\n",
      "1382912/1383145 [============================>.] - ETA: 0s - loss: 1.0494 - accuracy: 0.6645\n",
      "Epoch 00024: loss improved from 1.05202 to 1.04939, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 97s 70us/sample - loss: 1.0494 - accuracy: 0.6645\n",
      "Epoch 25/500\n",
      "1382400/1383145 [============================>.] - ETA: 0s - loss: 1.0473 - accuracy: 0.6655\n",
      "Epoch 00025: loss improved from 1.04939 to 1.04728, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 96s 69us/sample - loss: 1.0473 - accuracy: 0.6655\n",
      "Epoch 26/500\n",
      "1383040/1383145 [============================>.] - ETA: 0s - loss: 1.0446 - accuracy: 0.6666\n",
      "Epoch 00026: loss improved from 1.04728 to 1.04463, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 95s 69us/sample - loss: 1.0446 - accuracy: 0.6666\n",
      "Epoch 27/500\n",
      "1382784/1383145 [============================>.] - ETA: 0s - loss: 1.0427 - accuracy: 0.6667\n",
      "Epoch 00027: loss improved from 1.04463 to 1.04274, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 97s 70us/sample - loss: 1.0427 - accuracy: 0.6667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "1383040/1383145 [============================>.] - ETA: 0s - loss: 1.0407 - accuracy: 0.6678\n",
      "Epoch 00028: loss improved from 1.04274 to 1.04070, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 97s 70us/sample - loss: 1.0407 - accuracy: 0.6678\n",
      "Epoch 29/500\n",
      "1382528/1383145 [============================>.] - ETA: 0s - loss: 1.0388 - accuracy: 0.6679\n",
      "Epoch 00029: loss improved from 1.04070 to 1.03872, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 95s 69us/sample - loss: 1.0387 - accuracy: 0.6679\n",
      "Epoch 30/500\n",
      "1382656/1383145 [============================>.] - ETA: 0s - loss: 1.0370 - accuracy: 0.6687\n",
      "Epoch 00030: loss improved from 1.03872 to 1.03696, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 94s 68us/sample - loss: 1.0370 - accuracy: 0.6687\n",
      "Epoch 31/500\n",
      "1383040/1383145 [============================>.] - ETA: 0s - loss: 1.0357 - accuracy: 0.6693\n",
      "Epoch 00031: loss improved from 1.03696 to 1.03572, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 97s 70us/sample - loss: 1.0357 - accuracy: 0.6693\n",
      "Epoch 32/500\n",
      "1383040/1383145 [============================>.] - ETA: 0s - loss: 1.0340 - accuracy: 0.6697\n",
      "Epoch 00032: loss improved from 1.03572 to 1.03397, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 97s 70us/sample - loss: 1.0340 - accuracy: 0.6697\n",
      "Epoch 33/500\n",
      "1382656/1383145 [============================>.] - ETA: 0s - loss: 1.0326 - accuracy: 0.6704\n",
      "Epoch 00033: loss improved from 1.03397 to 1.03254, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 95s 69us/sample - loss: 1.0325 - accuracy: 0.6704\n",
      "Epoch 34/500\n",
      "1382528/1383145 [============================>.] - ETA: 0s - loss: 1.0306 - accuracy: 0.6711\n",
      "Epoch 00034: loss improved from 1.03254 to 1.03060, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 97s 70us/sample - loss: 1.0306 - accuracy: 0.6711\n",
      "Epoch 35/500\n",
      "1382656/1383145 [============================>.] - ETA: 0s - loss: 1.0297 - accuracy: 0.6714\n",
      "Epoch 00035: loss improved from 1.03060 to 1.02969, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 95s 69us/sample - loss: 1.0297 - accuracy: 0.6714\n",
      "Epoch 36/500\n",
      "1382400/1383145 [============================>.] - ETA: 0s - loss: 1.0280 - accuracy: 0.6722\n",
      "Epoch 00036: loss improved from 1.02969 to 1.02803, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 96s 70us/sample - loss: 1.0280 - accuracy: 0.6722\n",
      "Epoch 37/500\n",
      "1382528/1383145 [============================>.] - ETA: 0s - loss: 1.0270 - accuracy: 0.6724\n",
      "Epoch 00037: loss improved from 1.02803 to 1.02704, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 95s 69us/sample - loss: 1.0270 - accuracy: 0.6724\n",
      "Epoch 38/500\n",
      "1382784/1383145 [============================>.] - ETA: 0s - loss: 1.0253 - accuracy: 0.6728\n",
      "Epoch 00038: loss improved from 1.02704 to 1.02532, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 94s 68us/sample - loss: 1.0253 - accuracy: 0.6728\n",
      "Epoch 39/500\n",
      "1382656/1383145 [============================>.] - ETA: 0s - loss: 1.0243 - accuracy: 0.6732\n",
      "Epoch 00039: loss improved from 1.02532 to 1.02427, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 96s 70us/sample - loss: 1.0243 - accuracy: 0.6732\n",
      "Epoch 40/500\n",
      "1382784/1383145 [============================>.] - ETA: 0s - loss: 1.0230 - accuracy: 0.6737\n",
      "Epoch 00040: loss improved from 1.02427 to 1.02294, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 95s 69us/sample - loss: 1.0229 - accuracy: 0.6737\n",
      "Epoch 41/500\n",
      "1382912/1383145 [============================>.] - ETA: 0s - loss: 1.0220 - accuracy: 0.6740\n",
      "Epoch 00041: loss improved from 1.02294 to 1.02200, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 104s 75us/sample - loss: 1.0220 - accuracy: 0.6739\n",
      "Epoch 42/500\n",
      "1382912/1383145 [============================>.] - ETA: 0s - loss: 1.0212 - accuracy: 0.6740\n",
      "Epoch 00042: loss improved from 1.02200 to 1.02120, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 95s 69us/sample - loss: 1.0212 - accuracy: 0.6740\n",
      "Epoch 43/500\n",
      "1383040/1383145 [============================>.] - ETA: 0s - loss: 1.0199 - accuracy: 0.6748\n",
      "Epoch 00043: loss improved from 1.02120 to 1.01992, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 94s 68us/sample - loss: 1.0199 - accuracy: 0.6748\n",
      "Epoch 44/500\n",
      "1382912/1383145 [============================>.] - ETA: 0s - loss: 1.0191 - accuracy: 0.6748\n",
      "Epoch 00044: loss improved from 1.01992 to 1.01912, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 97s 70us/sample - loss: 1.0191 - accuracy: 0.6748\n",
      "Epoch 45/500\n",
      "1382656/1383145 [============================>.] - ETA: 0s - loss: 1.0181 - accuracy: 0.6752\n",
      "Epoch 00045: loss improved from 1.01912 to 1.01803, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 96s 70us/sample - loss: 1.0180 - accuracy: 0.6753\n",
      "Epoch 46/500\n",
      "1382784/1383145 [============================>.] - ETA: 0s - loss: 1.0175 - accuracy: 0.6754\n",
      "Epoch 00046: loss improved from 1.01803 to 1.01745, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 95s 68us/sample - loss: 1.0175 - accuracy: 0.6754\n",
      "Epoch 47/500\n",
      "1382912/1383145 [============================>.] - ETA: 0s - loss: 1.0167 - accuracy: 0.6757\n",
      "Epoch 00047: loss improved from 1.01745 to 1.01668, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 95s 68us/sample - loss: 1.0167 - accuracy: 0.6757\n",
      "Epoch 48/500\n",
      "1382912/1383145 [============================>.] - ETA: 0s - loss: 1.0161 - accuracy: 0.6761\n",
      "Epoch 00048: loss improved from 1.01668 to 1.01614, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 96s 69us/sample - loss: 1.0161 - accuracy: 0.6761\n",
      "Epoch 49/500\n",
      "1382528/1383145 [============================>.] - ETA: 0s - loss: 1.0151 - accuracy: 0.6760\n",
      "Epoch 00049: loss improved from 1.01614 to 1.01508, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 96s 69us/sample - loss: 1.0151 - accuracy: 0.6761\n",
      "Epoch 50/500\n",
      "1383040/1383145 [============================>.] - ETA: 0s - loss: 1.0144 - accuracy: 0.6767\n",
      "Epoch 00050: loss improved from 1.01508 to 1.01437, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 108s 78us/sample - loss: 1.0144 - accuracy: 0.6767\n",
      "Epoch 51/500\n",
      "1382528/1383145 [============================>.] - ETA: 0s - loss: 1.0138 - accuracy: 0.6765\n",
      "Epoch 00051: loss improved from 1.01437 to 1.01378, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 118s 86us/sample - loss: 1.0138 - accuracy: 0.6765\n",
      "Epoch 52/500\n",
      "1383040/1383145 [============================>.] - ETA: 0s - loss: 1.0130 - accuracy: 0.6771\n",
      "Epoch 00052: loss improved from 1.01378 to 1.01296, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 101s 73us/sample - loss: 1.0130 - accuracy: 0.6771\n",
      "Epoch 53/500\n",
      "1383040/1383145 [============================>.] - ETA: 0s - loss: 1.0125 - accuracy: 0.6769\n",
      "Epoch 00053: loss improved from 1.01296 to 1.01244, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 98s 71us/sample - loss: 1.0124 - accuracy: 0.6769\n",
      "Epoch 54/500\n",
      "1382400/1383145 [============================>.] - ETA: 0s - loss: 1.0117 - accuracy: 0.6772\n",
      "Epoch 00054: loss improved from 1.01244 to 1.01170, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 98s 71us/sample - loss: 1.0117 - accuracy: 0.6772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "1382784/1383145 [============================>.] - ETA: 0s - loss: 1.0113 - accuracy: 0.6776\n",
      "Epoch 00055: loss improved from 1.01170 to 1.01128, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 98s 71us/sample - loss: 1.0113 - accuracy: 0.6775\n",
      "Epoch 56/500\n",
      "1383040/1383145 [============================>.] - ETA: 0s - loss: 1.0108 - accuracy: 0.6777\n",
      "Epoch 00056: loss improved from 1.01128 to 1.01081, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 107s 77us/sample - loss: 1.0108 - accuracy: 0.6777\n",
      "Epoch 57/500\n",
      "1382912/1383145 [============================>.] - ETA: 0s - loss: 1.0100 - accuracy: 0.6781\n",
      "Epoch 00057: loss improved from 1.01081 to 1.00997, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 98s 71us/sample - loss: 1.0100 - accuracy: 0.6781\n",
      "Epoch 58/500\n",
      "1382400/1383145 [============================>.] - ETA: 0s - loss: 1.0094 - accuracy: 0.6780\n",
      "Epoch 00058: loss improved from 1.00997 to 1.00942, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 102s 74us/sample - loss: 1.0094 - accuracy: 0.6779\n",
      "Epoch 59/500\n",
      "1382656/1383145 [============================>.] - ETA: 0s - loss: 1.0085 - accuracy: 0.6782\n",
      "Epoch 00059: loss improved from 1.00942 to 1.00847, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 108s 78us/sample - loss: 1.0085 - accuracy: 0.6782\n",
      "Epoch 60/500\n",
      "1382656/1383145 [============================>.] - ETA: 0s - loss: 1.0083 - accuracy: 0.6783\n",
      "Epoch 00060: loss improved from 1.00847 to 1.00832, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 110s 80us/sample - loss: 1.0083 - accuracy: 0.6782\n",
      "Epoch 61/500\n",
      "1382912/1383145 [============================>.] - ETA: 0s - loss: 1.0082 - accuracy: 0.6784\n",
      "Epoch 00061: loss improved from 1.00832 to 1.00813, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 110s 79us/sample - loss: 1.0081 - accuracy: 0.6784\n",
      "Epoch 62/500\n",
      "1382528/1383145 [============================>.] - ETA: 0s - loss: 1.0075 - accuracy: 0.6785\n",
      "Epoch 00062: loss improved from 1.00813 to 1.00752, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 116s 84us/sample - loss: 1.0075 - accuracy: 0.6785\n",
      "Epoch 63/500\n",
      "1382400/1383145 [============================>.] - ETA: 0s - loss: 1.0069 - accuracy: 0.6789\n",
      "Epoch 00063: loss improved from 1.00752 to 1.00694, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 101s 73us/sample - loss: 1.0069 - accuracy: 0.6789\n",
      "Epoch 64/500\n",
      "1382912/1383145 [============================>.] - ETA: 0s - loss: 1.0063 - accuracy: 0.6788\n",
      "Epoch 00064: loss improved from 1.00694 to 1.00629, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 104s 75us/sample - loss: 1.0063 - accuracy: 0.6788\n",
      "Epoch 65/500\n",
      "1383040/1383145 [============================>.] - ETA: 0s - loss: 1.0060 - accuracy: 0.6791\n",
      "Epoch 00065: loss improved from 1.00629 to 1.00603, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 105s 76us/sample - loss: 1.0060 - accuracy: 0.6791\n",
      "Epoch 66/500\n",
      "1382400/1383145 [============================>.] - ETA: 0s - loss: 1.0055 - accuracy: 0.6795\n",
      "Epoch 00066: loss improved from 1.00603 to 1.00552, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 95s 69us/sample - loss: 1.0055 - accuracy: 0.6795\n",
      "Epoch 67/500\n",
      "1382784/1383145 [============================>.] - ETA: 0s - loss: 1.0044 - accuracy: 0.6797\n",
      "Epoch 00067: loss improved from 1.00552 to 1.00446, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 112s 81us/sample - loss: 1.0045 - accuracy: 0.6797\n",
      "Epoch 68/500\n",
      "1382528/1383145 [============================>.] - ETA: 0s - loss: 1.0045 - accuracy: 0.6796\n",
      "Epoch 00068: loss did not improve from 1.00446\n",
      "1383145/1383145 [==============================] - 110s 79us/sample - loss: 1.0046 - accuracy: 0.6796\n",
      "Epoch 69/500\n",
      "1382784/1383145 [============================>.] - ETA: 0s - loss: 1.0041 - accuracy: 0.6798\n",
      "Epoch 00069: loss improved from 1.00446 to 1.00415, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 107s 77us/sample - loss: 1.0041 - accuracy: 0.6798\n",
      "Epoch 70/500\n",
      "1382528/1383145 [============================>.] - ETA: 0s - loss: 1.0031 - accuracy: 0.6803\n",
      "Epoch 00070: loss improved from 1.00415 to 1.00315, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 112s 81us/sample - loss: 1.0031 - accuracy: 0.6803\n",
      "Epoch 71/500\n",
      "1382784/1383145 [============================>.] - ETA: 0s - loss: 1.0034 - accuracy: 0.6804\n",
      "Epoch 00071: loss did not improve from 1.00315\n",
      "1383145/1383145 [==============================] - 98s 71us/sample - loss: 1.0034 - accuracy: 0.6804\n",
      "Epoch 72/500\n",
      "1382656/1383145 [============================>.] - ETA: 0s - loss: 1.0025 - accuracy: 0.6805\n",
      "Epoch 00072: loss improved from 1.00315 to 1.00256, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 105s 76us/sample - loss: 1.0026 - accuracy: 0.6805\n",
      "Epoch 73/500\n",
      "1382656/1383145 [============================>.] - ETA: 0s - loss: 1.0023 - accuracy: 0.6804\n",
      "Epoch 00073: loss improved from 1.00256 to 1.00232, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 119s 86us/sample - loss: 1.0023 - accuracy: 0.6804\n",
      "Epoch 74/500\n",
      "1382784/1383145 [============================>.] - ETA: 0s - loss: 1.0017 - accuracy: 0.6807\n",
      "Epoch 00074: loss improved from 1.00232 to 1.00173, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 123s 89us/sample - loss: 1.0017 - accuracy: 0.6807\n",
      "Epoch 75/500\n",
      "1382912/1383145 [============================>.] - ETA: 0s - loss: 1.0015 - accuracy: 0.6807\n",
      "Epoch 00075: loss improved from 1.00173 to 1.00147, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 107s 77us/sample - loss: 1.0015 - accuracy: 0.6807\n",
      "Epoch 76/500\n",
      "1382400/1383145 [============================>.] - ETA: 0s - loss: 1.0013 - accuracy: 0.6810\n",
      "Epoch 00076: loss improved from 1.00147 to 1.00125, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 97s 70us/sample - loss: 1.0013 - accuracy: 0.6810\n",
      "Epoch 77/500\n",
      "1383040/1383145 [============================>.] - ETA: 0s - loss: 1.0005 - accuracy: 0.6811\n",
      "Epoch 00077: loss improved from 1.00125 to 1.00047, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 108s 78us/sample - loss: 1.0005 - accuracy: 0.6811\n",
      "Epoch 78/500\n",
      "1382528/1383145 [============================>.] - ETA: 0s - loss: 1.0004 - accuracy: 0.6814\n",
      "Epoch 00078: loss did not improve from 1.00047\n",
      "1383145/1383145 [==============================] - 105s 76us/sample - loss: 1.0005 - accuracy: 0.6814\n",
      "Epoch 79/500\n",
      "1382400/1383145 [============================>.] - ETA: 0s - loss: 1.0003 - accuracy: 0.6814\n",
      "Epoch 00079: loss improved from 1.00047 to 1.00025, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 99s 71us/sample - loss: 1.0002 - accuracy: 0.6815\n",
      "Epoch 80/500\n",
      "1382784/1383145 [============================>.] - ETA: 0s - loss: 0.9997 - accuracy: 0.6813\n",
      "Epoch 00080: loss improved from 1.00025 to 0.99971, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 100s 72us/sample - loss: 0.9997 - accuracy: 0.6813\n",
      "Epoch 81/500\n",
      "1382784/1383145 [============================>.] - ETA: 0s - loss: 0.9998 - accuracy: 0.6813\n",
      "Epoch 00081: loss did not improve from 0.99971\n",
      "1383145/1383145 [==============================] - 99s 71us/sample - loss: 0.9998 - accuracy: 0.6813\n",
      "Epoch 82/500\n",
      "1382912/1383145 [============================>.] - ETA: 0s - loss: 0.9990 - accuracy: 0.6818\n",
      "Epoch 00082: loss improved from 0.99971 to 0.99896, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 105s 76us/sample - loss: 0.9990 - accuracy: 0.6818\n",
      "Epoch 83/500\n",
      "1382656/1383145 [============================>.] - ETA: 0s - loss: 0.9989 - accuracy: 0.6816\n",
      "Epoch 00083: loss improved from 0.99896 to 0.99888, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 115s 83us/sample - loss: 0.9989 - accuracy: 0.6816\n",
      "Epoch 84/500\n",
      "1382656/1383145 [============================>.] - ETA: 0s - loss: 0.9985 - accuracy: 0.6818\n",
      "Epoch 00084: loss improved from 0.99888 to 0.99849, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 123s 89us/sample - loss: 0.9985 - accuracy: 0.6818\n",
      "Epoch 85/500\n",
      "1382528/1383145 [============================>.] - ETA: 0s - loss: 0.9977 - accuracy: 0.6822\n",
      "Epoch 00085: loss improved from 0.99849 to 0.99767, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 124s 90us/sample - loss: 0.9977 - accuracy: 0.6822\n",
      "Epoch 86/500\n",
      "1383040/1383145 [============================>.] - ETA: 0s - loss: 0.9978 - accuracy: 0.6821\n",
      "Epoch 00086: loss did not improve from 0.99767\n",
      "1383145/1383145 [==============================] - 113s 82us/sample - loss: 0.9978 - accuracy: 0.6821\n",
      "Epoch 87/500\n",
      "1382912/1383145 [============================>.] - ETA: 0s - loss: 0.9972 - accuracy: 0.6823\n",
      "Epoch 00087: loss improved from 0.99767 to 0.99719, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 107s 78us/sample - loss: 0.9972 - accuracy: 0.6823\n",
      "Epoch 88/500\n",
      "1383040/1383145 [============================>.] - ETA: 0s - loss: 0.9970 - accuracy: 0.6823\n",
      "Epoch 00088: loss improved from 0.99719 to 0.99698, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 103s 74us/sample - loss: 0.9970 - accuracy: 0.6823\n",
      "Epoch 89/500\n",
      "1382528/1383145 [============================>.] - ETA: 0s - loss: 0.9968 - accuracy: 0.6824\n",
      "Epoch 00089: loss improved from 0.99698 to 0.99674, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 116s 84us/sample - loss: 0.9967 - accuracy: 0.6824\n",
      "Epoch 90/500\n",
      "1382656/1383145 [============================>.] - ETA: 0s - loss: 0.9964 - accuracy: 0.6828\n",
      "Epoch 00090: loss improved from 0.99674 to 0.99644, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 106s 76us/sample - loss: 0.9964 - accuracy: 0.6828\n",
      "Epoch 91/500\n",
      "1382528/1383145 [============================>.] - ETA: 0s - loss: 0.9963 - accuracy: 0.6827\n",
      "Epoch 00091: loss improved from 0.99644 to 0.99628, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 98s 71us/sample - loss: 0.9963 - accuracy: 0.6827\n",
      "Epoch 92/500\n",
      "1382912/1383145 [============================>.] - ETA: 0s - loss: 0.9959 - accuracy: 0.6828\n",
      "Epoch 00092: loss improved from 0.99628 to 0.99585, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 102s 74us/sample - loss: 0.9958 - accuracy: 0.6828\n",
      "Epoch 93/500\n",
      "1382912/1383145 [============================>.] - ETA: 0s - loss: 0.9956 - accuracy: 0.6826\n",
      "Epoch 00093: loss improved from 0.99585 to 0.99562, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 102s 73us/sample - loss: 0.9956 - accuracy: 0.6826\n",
      "Epoch 94/500\n",
      "1382784/1383145 [============================>.] - ETA: 0s - loss: 0.9953 - accuracy: 0.6830\n",
      "Epoch 00094: loss improved from 0.99562 to 0.99530, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 111s 80us/sample - loss: 0.9953 - accuracy: 0.6830\n",
      "Epoch 95/500\n",
      "1382400/1383145 [============================>.] - ETA: 0s - loss: 0.9952 - accuracy: 0.6830\n",
      "Epoch 00095: loss improved from 0.99530 to 0.99514, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 114s 82us/sample - loss: 0.9951 - accuracy: 0.6830\n",
      "Epoch 96/500\n",
      "1383040/1383145 [============================>.] - ETA: 0s - loss: 0.9946 - accuracy: 0.6833\n",
      "Epoch 00096: loss improved from 0.99514 to 0.99457, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 112s 81us/sample - loss: 0.9946 - accuracy: 0.6833\n",
      "Epoch 97/500\n",
      "1383040/1383145 [============================>.] - ETA: 0s - loss: 0.9943 - accuracy: 0.6833\n",
      "Epoch 00097: loss improved from 0.99457 to 0.99426, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 115s 83us/sample - loss: 0.9943 - accuracy: 0.6833\n",
      "Epoch 98/500\n",
      "1382656/1383145 [============================>.] - ETA: 0s - loss: 0.9942 - accuracy: 0.6836\n",
      "Epoch 00098: loss improved from 0.99426 to 0.99417, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 111s 81us/sample - loss: 0.9942 - accuracy: 0.6836\n",
      "Epoch 99/500\n",
      "1383040/1383145 [============================>.] - ETA: 0s - loss: 0.9939 - accuracy: 0.6835\n",
      "Epoch 00099: loss improved from 0.99417 to 0.99388, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 106s 77us/sample - loss: 0.9939 - accuracy: 0.6835\n",
      "Epoch 100/500\n",
      "1383040/1383145 [============================>.] - ETA: 0s - loss: 0.9938 - accuracy: 0.6834\n",
      "Epoch 00100: loss improved from 0.99388 to 0.99377, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 114s 82us/sample - loss: 0.9938 - accuracy: 0.6834\n",
      "Epoch 101/500\n",
      "1383040/1383145 [============================>.] - ETA: 0s - loss: 0.9937 - accuracy: 0.6835\n",
      "Epoch 00101: loss improved from 0.99377 to 0.99374, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 104s 75us/sample - loss: 0.9937 - accuracy: 0.6835\n",
      "Epoch 102/500\n",
      "1382656/1383145 [============================>.] - ETA: 0s - loss: 0.9936 - accuracy: 0.6830\n",
      "Epoch 00102: loss improved from 0.99374 to 0.99364, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 110s 79us/sample - loss: 0.9936 - accuracy: 0.6830\n",
      "Epoch 103/500\n",
      "1383040/1383145 [============================>.] - ETA: 0s - loss: 0.9931 - accuracy: 0.6837\n",
      "Epoch 00103: loss improved from 0.99364 to 0.99313, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 108s 78us/sample - loss: 0.9931 - accuracy: 0.6837\n",
      "Epoch 104/500\n",
      "1382528/1383145 [============================>.] - ETA: 0s - loss: 0.9932 - accuracy: 0.6836\n",
      "Epoch 00104: loss did not improve from 0.99313\n",
      "1383145/1383145 [==============================] - 99s 71us/sample - loss: 0.9932 - accuracy: 0.6836\n",
      "Epoch 105/500\n",
      "1382656/1383145 [============================>.] - ETA: 0s - loss: 0.9929 - accuracy: 0.6842\n",
      "Epoch 00105: loss improved from 0.99313 to 0.99286, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 113s 82us/sample - loss: 0.9929 - accuracy: 0.6842\n",
      "Epoch 106/500\n",
      "1382528/1383145 [============================>.] - ETA: 0s - loss: 0.9925 - accuracy: 0.6840\n",
      "Epoch 00106: loss improved from 0.99286 to 0.99253, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 118s 85us/sample - loss: 0.9925 - accuracy: 0.6840\n",
      "Epoch 107/500\n",
      "1382656/1383145 [============================>.] - ETA: 0s - loss: 0.9926 - accuracy: 0.6842\n",
      "Epoch 00107: loss did not improve from 0.99253\n",
      "1383145/1383145 [==============================] - 106s 77us/sample - loss: 0.9926 - accuracy: 0.6842\n",
      "Epoch 108/500\n",
      "1382784/1383145 [============================>.] - ETA: 0s - loss: 0.9926 - accuracy: 0.6838\n",
      "Epoch 00108: loss did not improve from 0.99253\n",
      "1383145/1383145 [==============================] - 108s 78us/sample - loss: 0.9926 - accuracy: 0.6838\n",
      "Epoch 109/500\n",
      "1382656/1383145 [============================>.] - ETA: 0s - loss: 0.9925 - accuracy: 0.6840\n",
      "Epoch 00109: loss improved from 0.99253 to 0.99249, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 111s 80us/sample - loss: 0.9925 - accuracy: 0.6840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/500\n",
      "1382656/1383145 [============================>.] - ETA: 0s - loss: 0.9919 - accuracy: 0.6842\n",
      "Epoch 00110: loss improved from 0.99249 to 0.99192, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 115s 83us/sample - loss: 0.9919 - accuracy: 0.6842\n",
      "Epoch 111/500\n",
      "1382400/1383145 [============================>.] - ETA: 0s - loss: 0.9921 - accuracy: 0.6842\n",
      "Epoch 00111: loss did not improve from 0.99192\n",
      "1383145/1383145 [==============================] - 100s 72us/sample - loss: 0.9921 - accuracy: 0.6842\n",
      "Epoch 112/500\n",
      "1383040/1383145 [============================>.] - ETA: 0s - loss: 0.9920 - accuracy: 0.6845\n",
      "Epoch 00112: loss did not improve from 0.99192\n",
      "1383145/1383145 [==============================] - 104s 75us/sample - loss: 0.9920 - accuracy: 0.6845\n",
      "Epoch 113/500\n",
      "1382656/1383145 [============================>.] - ETA: 0s - loss: 0.9914 - accuracy: 0.6844\n",
      "Epoch 00113: loss improved from 0.99192 to 0.99139, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 104s 75us/sample - loss: 0.9914 - accuracy: 0.6844\n",
      "Epoch 114/500\n",
      "1382784/1383145 [============================>.] - ETA: 0s - loss: 0.9917 - accuracy: 0.6846\n",
      "Epoch 00114: loss did not improve from 0.99139\n",
      "1383145/1383145 [==============================] - 103s 75us/sample - loss: 0.9917 - accuracy: 0.6846\n",
      "Epoch 115/500\n",
      "1382784/1383145 [============================>.] - ETA: 0s - loss: 0.9913 - accuracy: 0.6843\n",
      "Epoch 00115: loss improved from 0.99139 to 0.99124, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 122s 88us/sample - loss: 0.9912 - accuracy: 0.6843\n",
      "Epoch 116/500\n",
      "1382528/1383145 [============================>.] - ETA: 0s - loss: 0.9908 - accuracy: 0.6847\n",
      "Epoch 00116: loss improved from 0.99124 to 0.99084, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 100s 72us/sample - loss: 0.9908 - accuracy: 0.6847\n",
      "Epoch 117/500\n",
      "1382784/1383145 [============================>.] - ETA: 0s - loss: 0.9910 - accuracy: 0.6847\n",
      "Epoch 00117: loss did not improve from 0.99084\n",
      "1383145/1383145 [==============================] - 106s 77us/sample - loss: 0.9910 - accuracy: 0.6847\n",
      "Epoch 118/500\n",
      "1382400/1383145 [============================>.] - ETA: 0s - loss: 0.9909 - accuracy: 0.6845\n",
      "Epoch 00118: loss did not improve from 0.99084\n",
      "1383145/1383145 [==============================] - 115s 83us/sample - loss: 0.9909 - accuracy: 0.6844\n",
      "Epoch 119/500\n",
      "1382784/1383145 [============================>.] - ETA: 0s - loss: 0.9908 - accuracy: 0.6847\n",
      "Epoch 00119: loss improved from 0.99084 to 0.99078, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 99s 71us/sample - loss: 0.9908 - accuracy: 0.6847\n",
      "Epoch 120/500\n",
      "1382656/1383145 [============================>.] - ETA: 0s - loss: 0.9901 - accuracy: 0.6847\n",
      "Epoch 00120: loss improved from 0.99078 to 0.99007, saving model to models/enron_charRNN_0714.h5\n",
      "1383145/1383145 [==============================] - 105s 76us/sample - loss: 0.9901 - accuracy: 0.6847\n",
      "Epoch 121/500\n",
      "1382912/1383145 [============================>.] - ETA: 0s - loss: 0.9901 - accuracy: 0.6847\n",
      "Epoch 00121: loss did not improve from 0.99007\n",
      "1383145/1383145 [==============================] - 121s 88us/sample - loss: 0.9901 - accuracy: 0.6847\n",
      "Epoch 122/500\n",
      "1382528/1383145 [============================>.] - ETA: 0s - loss: 0.9902 - accuracy: 0.6850\n",
      "Epoch 00122: loss did not improve from 0.99007\n",
      "1383145/1383145 [==============================] - 105s 76us/sample - loss: 0.9901 - accuracy: 0.6850\n",
      "Epoch 123/500\n",
      "1382912/1383145 [============================>.] - ETA: 0s - loss: 0.9901 - accuracy: 0.6850\n",
      "Epoch 00123: loss did not improve from 0.99007\n",
      "1383145/1383145 [==============================] - 123s 89us/sample - loss: 0.9901 - accuracy: 0.6850\n",
      "Epoch 00123: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c35b643808>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "path = 'models/enron_charRNN_0714.h5'\n",
    "checkpoint = ModelCheckpoint(path, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "stopping = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=3)\n",
    "model.fit(train_inputs,train_targets,batch_size=128,epochs=500,verbose=1,callbacks=[checkpoint, stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(path)\n",
    "\n",
    "from pickle import dump\n",
    "dump(tokenizer,open('models/enron_tokenizer_charRNN_0714','wb'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "### Larger dataset\n",
    "\n",
    "Training results\n",
    "\n",
    "- Epoch 123/500 loss: 0.9901 - accuracy: 0.6850\n",
    "- Each training epoch takes 2.5-3 minutes on GPU (RTX 2070 SUPER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from pickle import load\n",
    "\n",
    "model = load_model('models/enron_charRNN_0714.h5')\n",
    "tokenizer = load(open('models/enron_tokenizer_charRNN_0714','rb'))\n",
    "seq_len = 9\n",
    "def gen_text(model, tokenizer, seq_len, seed_text, num_gen_words):\n",
    "    output_text = []\n",
    "    input_text = seed_text\n",
    "    for i in range(num_gen_words):\n",
    "        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len,truncating='pre')\n",
    "        pred_word_ind = model.predict_classes(pad_encoded,verbose=0)[0]\n",
    "        \n",
    "        pred_word = tokenizer.index_word[pred_word_ind]\n",
    "        input_text += ' '+pred_word\n",
    "        output_text.append(pred_word)\n",
    "    return ' '.join(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===>Enter --exit to exit from the program\n",
      "Enter string: enron\n",
      "Output: enron u p h e f\n",
      "\n",
      "Enter string: email\n",
      "Output: email   s u n a\n",
      "\n",
      "Enter string: discussions\n",
      "Output: discussions   s u n a\n",
      "\n",
      "Enter string: discuss\n",
      "Output: discuss   c a n a\n",
      "\n",
      "Enter string: discussio\n",
      "Output: discussio n e m a r\n",
      "\n",
      "Enter string: --exit\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\n===>Enter --exit to exit from the program')\n",
    "while True:\n",
    "    seed_text  = input('Enter string: ')\n",
    "    if seed_text.lower() == '--exit':\n",
    "        break\n",
    "    else:\n",
    "        out = gen_text(model, tokenizer, seq_len=seq_len, seed_text=seed_text, num_gen_words=5)\n",
    "        print('Output: '+seed_text+' '+out)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smaller dataset\n",
    "\n",
    "Training results:\n",
    "\n",
    "- Epoch 500/500 loss: 0.1652 - accuracy: 0.9321\n",
    "- Each epoch training takes 70-80 seconds on GPU (RTX 2070 SUPER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('models/outlook_charRNN_0713.h5')\n",
    "tokenizer = load(open('models/outlook_tokenizer_charRNN_0713','rb'))\n",
    "seq_len = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===>Enter --exit to exit from the program\n",
      "Enter string: acronis\n",
      "Output: acronis   l l o u\n",
      "\n",
      "Enter string: email\n",
      "Output: email   l o r o\n",
      "\n",
      "Enter string: discussions\n",
      "Output: discussions y o u n e\n",
      "\n",
      "Enter string: discuss\n",
      "Output: discuss   p t o r\n",
      "\n",
      "Enter string: discussio\n",
      "Output: discussio n p u n g\n",
      "\n",
      "Enter string: --exit\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\n===>Enter --exit to exit from the program')\n",
    "while True:\n",
    "    seed_text  = input('Enter string: ')\n",
    "    if seed_text.lower() == '--exit':\n",
    "        break\n",
    "    else:\n",
    "        out = gen_text(model, tokenizer, seq_len=seq_len, seed_text=seed_text, num_gen_words=5)\n",
    "        print('Output: '+seed_text+' '+out)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "- Seems to return individual letters with spaces \n",
    "    - Need to relook at the modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}