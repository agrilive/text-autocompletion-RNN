{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autosuggestion - Predicting the next word using RNN\n",
    "\n",
    "Adapted from: https://medium.com/towards-artificial-intelligence/sentence-prediction-using-word-level-lstm-text-generator-language-modeling-using-rnn-a80c4cda5b40\n",
    "\n",
    "### Approach\n",
    "\n",
    "- Test with bigger dataset (17000 sentences) vs smaller dataset (1700 sentences)\n",
    "    - Goal: to identify if a larger training set improves prediction accuracy\n",
    "    - Bigger dataset: sample of Enron email vs smaller dataset: personal Acronis email made up of mostly newsletters\n",
    "- Test with long query (>=3 words) short query (1 word)\n",
    "    - Goal: to identify if there is a gap in the RNN model in predicting the next word\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. Get sentence tokens and clean sentences\n",
    "2. Remove low frequency words and generate training sequences (4 words)\n",
    "3. Train model and predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "- Python version: Python 3.7.4\n",
    "- Set up virtual environment\n",
    "    - pip install --user virtualenv\n",
    "    - virtualenv tensorflow-gpu\n",
    "    - tensorflow-gpu\\Scripts\\activate\n",
    "- Add Jupyter Notebook to virtual environment\n",
    "    - pip install ipykernel\n",
    "    - python -m ipykernel install --name=tensorflow-gpu\n",
    "- Set up Tensorflow 2.1 GPU version\n",
    "    - https://www.tensorflow.org/install/gpu\n",
    "    - pip install tensorflow==2.1 \n",
    "    - CUDA 10.1, cuDNN 7.6.5\n",
    "- pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "Create txt file to save sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17256, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msg_sentence</th>\n",
       "      <th>msg_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Here is our forecast</td>\n",
       "      <td>here is our forecast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Traveling to have a business meeting takes the...</td>\n",
       "      <td>traveling to have a business meeting takes the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Especially if you have to prepare a presentation.</td>\n",
       "      <td>especially if you have to prepare a presentation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I would suggest holding the business plan meet...</td>\n",
       "      <td>i would suggest holding the business plan meet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I would even try and get some honest opinions ...</td>\n",
       "      <td>i would even try and get some honest opinions ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        msg_sentence  \\\n",
       "0                               Here is our forecast   \n",
       "1  Traveling to have a business meeting takes the...   \n",
       "2  Especially if you have to prepare a presentation.   \n",
       "3  I would suggest holding the business plan meet...   \n",
       "4  I would even try and get some honest opinions ...   \n",
       "\n",
       "                                           msg_clean  \n",
       "0                               here is our forecast  \n",
       "1  traveling to have a business meeting takes the...  \n",
       "2   especially if you have to prepare a presentation  \n",
       "3  i would suggest holding the business plan meet...  \n",
       "4  i would even try and get some honest opinions ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('enron_emails_clean_2500.csv')\n",
    "df.fillna('').dropna()\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.savetxt(r'enron_train', df.msg_clean.values, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"enron_train\", \"r\")\n",
    "if f.mode == 'r':\n",
    "    content = f.read()\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Cleaning the data\n",
    "clean_data = []\n",
    "for text in content.splitlines():\n",
    "    text = text.replace('_', ' ')\n",
    "    a = re.sub(r'[^a-zA-z ]+', '', text).strip()\n",
    "    if len(a)>0:\n",
    "        clean_data.append(a)\n",
    "    else:\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the lines which are to short or to long\n",
    "short_data = []\n",
    "for line in clean_data:\n",
    "    if 2 <= len(line.split()) <= 200:\n",
    "        short_data.append(line)\n",
    "    else:\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352973\n"
     ]
    }
   ],
   "source": [
    "# Counting the appearnce of each word in the corpus also calculates the number of unique words also\n",
    "word2count = {}\n",
    "total_words = 0\n",
    "for text in short_data:\n",
    "    for word in text.split():\n",
    "        if word not in word2count:\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] += 1\n",
    "        total_words += 1\n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2614\n"
     ]
    }
   ],
   "source": [
    "# creating a list that will only contain the words that appear more than 15 times\n",
    "word15 = []\n",
    "threshold = 15\n",
    "for word, count in word2count.items():\n",
    "    if count >= threshold:\n",
    "        if len(word) > 1:\n",
    "            word15.append(word)\n",
    "print(len(word15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the words from each string which appear less than 15 times\n",
    "data_15 = []\n",
    "for line in short_data:\n",
    "    str1=''\n",
    "    for word in line.split():\n",
    "        if word in word15:\n",
    "            str1 = \" \".join((str1, word))\n",
    "    data_15.append(str1.lstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the lines which are to short or to long after removing the unnecssary words.     \n",
    "short_data_consize = []\n",
    "for line in data_15:\n",
    "    if 3 <= len(line.split()) <= 200:\n",
    "        short_data_consize.append(line)\n",
    "    else:\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a function to save data\n",
    "def write_txt(name, data):\n",
    "    file1 = open(\"{0}.txt\".format(name),\"w\") \n",
    "    for line in data:\n",
    "        file1.writelines(line) \n",
    "        file1.writelines('\\n') \n",
    "    file1.close() #to change file access modes\n",
    "\n",
    "write_txt(name = 'enron_train_wordRNN', data = short_data_consize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filepath):\n",
    "    with open(filepath) as f:\n",
    "        str_text = f.read()\n",
    "    return str_text\n",
    "\n",
    "text = read_file('enron_train_wordRNN.txt')\n",
    "line = text.split(\"\\n\")\n",
    "token_lst = [x.split() for x in line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seq(tokens):\n",
    "    train_len = 3+1\n",
    "    text_sequences = []\n",
    "    for i in range(train_len,len(tokens)):\n",
    "        seq = tokens[i-train_len:i]\n",
    "        text_sequences.append(seq)\n",
    "    return text_sequences\n",
    "\n",
    "seq_lst = [generate_seq(x) for x in token_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = [item for sublist in token_lst for item in sublist]\n",
    "\n",
    "sequences = {}\n",
    "count = 1\n",
    "for i in range(len(flatten)):\n",
    "    if flatten[i] not in sequences:\n",
    "        sequences[flatten[i]] = count\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_seq = [item for sublist in seq_lst for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['meetings', 'think', 'it', 'would'],\n",
       " ['think', 'it', 'would', 'be'],\n",
       " ['it', 'would', 'be', 'more'],\n",
       " ['would', 'be', 'more', 'to'],\n",
       " ['be', 'more', 'to', 'try'],\n",
       " ['more', 'to', 'try', 'and'],\n",
       " ['to', 'try', 'and', 'discussions'],\n",
       " ['try', 'and', 'discussions', 'across'],\n",
       " ['and', 'discussions', 'across', 'the'],\n",
       " ['discussions', 'across', 'the', 'different']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten_seq[40:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sequences = flatten_seq\n",
    "train_len = 3+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_sequences)\n",
    "sequences = tokenizer.texts_to_sequences(text_sequences) \n",
    "\n",
    "#Collecting some information   \n",
    "vocabulary_size = len(tokenizer.word_counts)\n",
    "\n",
    "import numpy as np\n",
    "n_sequences = np.empty([len(sequences),train_len], dtype='int32')\n",
    "for i in range(len(sequences)):\n",
    "    n_sequences[i] = sequences[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "\n",
    "train_inputs = n_sequences[:,:-1]\n",
    "train_targets = n_sequences[:,-1]\n",
    "\n",
    "train_targets = to_categorical(train_targets, num_classes=vocabulary_size+1)\n",
    "seq_len = train_inputs.shape[1]\n",
    "train_inputs.shape\n",
    "\n",
    "def create_model(vocabulary_size, seq_len):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabulary_size, seq_len,input_length=seq_len))\n",
    "    model.add(LSTM(50,return_sequences=True))\n",
    "    model.add(LSTM(50))\n",
    "    model.add(Dense(50,activation='relu'))\n",
    "    model.add(Dense(vocabulary_size,activation='softmax'))\n",
    "    opt_adam = Adam(lr=0.001)\n",
    "    #You can simply pass 'adam' to optimizer in compile method. Default learning rate 0.001\n",
    "    #But here we are using adam optimzer from optimizer class to change the LR.\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=opt_adam,metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 3, 3)              7833      \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 3, 50)             10800     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2611)              133161    \n",
      "=================================================================\n",
      "Total params: 174,544\n",
      "Trainable params: 174,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vocabulary_size+1,seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 225490 samples\n",
      "Epoch 1/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.7081 - accuracy: 0.3149\n",
      "Epoch 00001: loss improved from inf to 3.70814, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 16s 70us/sample - loss: 3.7081 - accuracy: 0.3150\n",
      "Epoch 2/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.6880 - accuracy: 0.3156\n",
      "Epoch 00002: loss improved from 3.70814 to 3.68798, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 16s 69us/sample - loss: 3.6880 - accuracy: 0.3156\n",
      "Epoch 3/500\n",
      "224768/225490 [============================>.] - ETA: 0s - loss: 3.6787 - accuracy: 0.3163\n",
      "Epoch 00003: loss improved from 3.68798 to 3.67821, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 16s 71us/sample - loss: 3.6782 - accuracy: 0.3163\n",
      "Epoch 4/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.6717 - accuracy: 0.3164 ETA: 0s - loss: 3.6\n",
      "Epoch 00004: loss improved from 3.67821 to 3.67147, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 74us/sample - loss: 3.6715 - accuracy: 0.3164\n",
      "Epoch 5/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.6641 - accuracy: 0.3168\n",
      "Epoch 00005: loss improved from 3.67147 to 3.66437, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 81us/sample - loss: 3.6644 - accuracy: 0.3168\n",
      "Epoch 6/500\n",
      "224896/225490 [============================>.] - ETA: 0s - loss: 3.6590 - accuracy: 0.3181\n",
      "Epoch 00006: loss improved from 3.66437 to 3.65889, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 80us/sample - loss: 3.6589 - accuracy: 0.3182\n",
      "Epoch 7/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.6507 - accuracy: 0.3193\n",
      "Epoch 00007: loss improved from 3.65889 to 3.65064, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 80us/sample - loss: 3.6506 - accuracy: 0.3192\n",
      "Epoch 8/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.6448 - accuracy: 0.3206\n",
      "Epoch 00008: loss improved from 3.65064 to 3.64485, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 19s 83us/sample - loss: 3.6448 - accuracy: 0.3206\n",
      "Epoch 9/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.6401 - accuracy: 0.3200\n",
      "Epoch 00009: loss improved from 3.64485 to 3.63999, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 80us/sample - loss: 3.6400 - accuracy: 0.3200\n",
      "Epoch 10/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.6324 - accuracy: 0.3213\n",
      "Epoch 00010: loss improved from 3.63999 to 3.63288, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 80us/sample - loss: 3.6329 - accuracy: 0.3212\n",
      "Epoch 11/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.6275 - accuracy: 0.3225\n",
      "Epoch 00011: loss improved from 3.63288 to 3.62748, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 80us/sample - loss: 3.6275 - accuracy: 0.3225\n",
      "Epoch 12/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.6211 - accuracy: 0.3232\n",
      "Epoch 00012: loss improved from 3.62748 to 3.62125, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 81us/sample - loss: 3.6212 - accuracy: 0.3232\n",
      "Epoch 13/500\n",
      "224896/225490 [============================>.] - ETA: 0s - loss: 3.6161 - accuracy: 0.3245\n",
      "Epoch 00013: loss improved from 3.62125 to 3.61626, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 80us/sample - loss: 3.6163 - accuracy: 0.3245\n",
      "Epoch 14/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.6103 - accuracy: 0.3252\n",
      "Epoch 00014: loss improved from 3.61626 to 3.61038, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 81us/sample - loss: 3.6104 - accuracy: 0.3252\n",
      "Epoch 15/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.6047 - accuracy: 0.3255\n",
      "Epoch 00015: loss improved from 3.61038 to 3.60458, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 19s 85us/sample - loss: 3.6046 - accuracy: 0.3256\n",
      "Epoch 16/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.6004 - accuracy: 0.3262\n",
      "Epoch 00016: loss improved from 3.60458 to 3.60010, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 81us/sample - loss: 3.6001 - accuracy: 0.3263\n",
      "Epoch 17/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.5950 - accuracy: 0.3276\n",
      "Epoch 00017: loss improved from 3.60010 to 3.59488, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 82us/sample - loss: 3.5949 - accuracy: 0.3276\n",
      "Epoch 18/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.5903 - accuracy: 0.3278\n",
      "Epoch 00018: loss improved from 3.59488 to 3.59034, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 80us/sample - loss: 3.5903 - accuracy: 0.3278\n",
      "Epoch 19/500\n",
      "224896/225490 [============================>.] - ETA: 0s - loss: 3.5838 - accuracy: 0.3283\n",
      "Epoch 00019: loss improved from 3.59034 to 3.58417, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 81us/sample - loss: 3.5842 - accuracy: 0.3283\n",
      "Epoch 20/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.5807 - accuracy: 0.3295\n",
      "Epoch 00020: loss improved from 3.58417 to 3.58068, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 81us/sample - loss: 3.5807 - accuracy: 0.3295\n",
      "Epoch 21/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.5743 - accuracy: 0.3304\n",
      "Epoch 00021: loss improved from 3.58068 to 3.57416, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 80us/sample - loss: 3.5742 - accuracy: 0.3304\n",
      "Epoch 22/500\n",
      "224896/225490 [============================>.] - ETA: 0s - loss: 3.5715 - accuracy: 0.3306\n",
      "Epoch 00022: loss improved from 3.57416 to 3.57157, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 81us/sample - loss: 3.5716 - accuracy: 0.3306\n",
      "Epoch 23/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.5653 - accuracy: 0.3326\n",
      "Epoch 00023: loss improved from 3.57157 to 3.56571, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 80us/sample - loss: 3.5657 - accuracy: 0.3325\n",
      "Epoch 24/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.5596 - accuracy: 0.3319\n",
      "Epoch 00024: loss improved from 3.56571 to 3.55953, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 81us/sample - loss: 3.5595 - accuracy: 0.3319\n",
      "Epoch 25/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.5565 - accuracy: 0.3328\n",
      "Epoch 00025: loss improved from 3.55953 to 3.55684, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 80us/sample - loss: 3.5568 - accuracy: 0.3328\n",
      "Epoch 26/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.5513 - accuracy: 0.3342\n",
      "Epoch 00026: loss improved from 3.55684 to 3.55117, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 81us/sample - loss: 3.5512 - accuracy: 0.3342\n",
      "Epoch 27/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.5469 - accuracy: 0.3343\n",
      "Epoch 00027: loss improved from 3.55117 to 3.54663, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 19s 84us/sample - loss: 3.5466 - accuracy: 0.3343\n",
      "Epoch 28/500\n",
      "224768/225490 [============================>.] - ETA: 0s - loss: 3.5417 - accuracy: 0.3352\n",
      "Epoch 00028: loss improved from 3.54663 to 3.54198, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.5420 - accuracy: 0.3352\n",
      "Epoch 29/500\n",
      "224896/225490 [============================>.] - ETA: 0s - loss: 3.5379 - accuracy: 0.3357\n",
      "Epoch 00029: loss improved from 3.54198 to 3.53844, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 80us/sample - loss: 3.5384 - accuracy: 0.3357\n",
      "Epoch 30/500\n",
      "224896/225490 [============================>.] - ETA: 0s - loss: 3.5324 - accuracy: 0.3369\n",
      "Epoch 00030: loss improved from 3.53844 to 3.53303, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 21s 94us/sample - loss: 3.5330 - accuracy: 0.3368\n",
      "Epoch 31/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.5294 - accuracy: 0.3372\n",
      "Epoch 00031: loss improved from 3.53303 to 3.52931, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 20s 89us/sample - loss: 3.5293 - accuracy: 0.3372\n",
      "Epoch 32/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.5251 - accuracy: 0.3376\n",
      "Epoch 00032: loss improved from 3.52931 to 3.52511, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 19s 82us/sample - loss: 3.5251 - accuracy: 0.3376\n",
      "Epoch 33/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.5208 - accuracy: 0.3390\n",
      "Epoch 00033: loss improved from 3.52511 to 3.52031, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.5203 - accuracy: 0.3391\n",
      "Epoch 34/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.5169 - accuracy: 0.3386\n",
      "Epoch 00034: loss improved from 3.52031 to 3.51660, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.5166 - accuracy: 0.3386\n",
      "Epoch 35/500\n",
      "224896/225490 [============================>.] - ETA: 0s - loss: 3.5105 - accuracy: 0.3405\n",
      "Epoch 00035: loss improved from 3.51660 to 3.51097, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 19s 86us/sample - loss: 3.5110 - accuracy: 0.3404\n",
      "Epoch 36/500\n",
      "224896/225490 [============================>.] - ETA: 0s - loss: 3.5085 - accuracy: 0.3397\n",
      "Epoch 00036: loss improved from 3.51097 to 3.50842, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.5084 - accuracy: 0.3397\n",
      "Epoch 37/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.5035 - accuracy: 0.34 - ETA: 0s - loss: 3.5036 - accuracy: 0.3410\n",
      "Epoch 00037: loss improved from 3.50842 to 3.50343, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 78us/sample - loss: 3.5034 - accuracy: 0.3411\n",
      "Epoch 38/500\n",
      "224768/225490 [============================>.] - ETA: 0s - loss: 3.4999 - accuracy: 0.3417\n",
      "Epoch 00038: loss improved from 3.50343 to 3.50028, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.5003 - accuracy: 0.3416\n",
      "Epoch 39/500\n",
      "224896/225490 [============================>.] - ETA: 0s - loss: 3.4958 - accuracy: 0.3420\n",
      "Epoch 00039: loss improved from 3.50028 to 3.49596, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.4960 - accuracy: 0.3420\n",
      "Epoch 40/500\n",
      "224896/225490 [============================>.] - ETA: 0s - loss: 3.4915 - accuracy: 0.3429\n",
      "Epoch 00040: loss improved from 3.49596 to 3.49189, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.4919 - accuracy: 0.3429\n",
      "Epoch 41/500\n",
      "224896/225490 [============================>.] - ETA: 0s - loss: 3.4887 - accuracy: 0.3434\n",
      "Epoch 00041: loss improved from 3.49189 to 3.48891, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 78us/sample - loss: 3.4889 - accuracy: 0.3433\n",
      "Epoch 42/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.4850 - accuracy: 0.3439\n",
      "Epoch 00042: loss improved from 3.48891 to 3.48527, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.4853 - accuracy: 0.3439\n",
      "Epoch 43/500\n",
      "224896/225490 [============================>.] - ETA: 0s - loss: 3.4804 - accuracy: 0.3451\n",
      "Epoch 00043: loss improved from 3.48527 to 3.48054, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.4805 - accuracy: 0.3451\n",
      "Epoch 44/500\n",
      "224896/225490 [============================>.] - ETA: 0s - loss: 3.4759 - accuracy: 0.3451\n",
      "Epoch 00044: loss improved from 3.48054 to 3.47623, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.4762 - accuracy: 0.3451\n",
      "Epoch 45/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.4722 - accuracy: 0.3465\n",
      "Epoch 00045: loss improved from 3.47623 to 3.47207, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.4721 - accuracy: 0.3465\n",
      "Epoch 46/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.4695 - accuracy: 0.3464\n",
      "Epoch 00046: loss improved from 3.47207 to 3.46996, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 78us/sample - loss: 3.4700 - accuracy: 0.3464\n",
      "Epoch 47/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.4656 - accuracy: 0.3462\n",
      "Epoch 00047: loss improved from 3.46996 to 3.46566, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.4657 - accuracy: 0.3462\n",
      "Epoch 48/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.4630 - accuracy: 0.3475\n",
      "Epoch 00048: loss improved from 3.46566 to 3.46287, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.4629 - accuracy: 0.3475\n",
      "Epoch 49/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.4602 - accuracy: 0.3478\n",
      "Epoch 00049: loss improved from 3.46287 to 3.45972, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.4597 - accuracy: 0.3479\n",
      "Epoch 50/500\n",
      "224768/225490 [============================>.] - ETA: 0s - loss: 3.4556 - accuracy: 0.3487\n",
      "Epoch 00050: loss improved from 3.45972 to 3.45599, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.4560 - accuracy: 0.3485\n",
      "Epoch 51/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.4523 - accuracy: 0.3498\n",
      "Epoch 00051: loss improved from 3.45599 to 3.45222, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.4522 - accuracy: 0.3498\n",
      "Epoch 52/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.4478 - accuracy: 0.3493\n",
      "Epoch 00052: loss improved from 3.45222 to 3.44777, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.4478 - accuracy: 0.3493\n",
      "Epoch 53/500\n",
      "224768/225490 [============================>.] - ETA: 0s - loss: 3.4457 - accuracy: 0.3495\n",
      "Epoch 00053: loss improved from 3.44777 to 3.44622, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.4462 - accuracy: 0.3495\n",
      "Epoch 54/500\n",
      "224768/225490 [============================>.] - ETA: 0s - loss: 3.4421 - accuracy: 0.3508\n",
      "Epoch 00054: loss improved from 3.44622 to 3.44181, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.4418 - accuracy: 0.3508\n",
      "Epoch 55/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.4387 - accuracy: 0.3513\n",
      "Epoch 00055: loss improved from 3.44181 to 3.43867, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.4387 - accuracy: 0.3513\n",
      "Epoch 56/500\n",
      "224896/225490 [============================>.] - ETA: 0s - loss: 3.4347 - accuracy: 0.3520 ETA: 0s - loss: 3.4343 - accuracy: \n",
      "Epoch 00056: loss improved from 3.43867 to 3.43449, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.4345 - accuracy: 0.3520\n",
      "Epoch 57/500\n",
      "224768/225490 [============================>.] - ETA: 0s - loss: 3.4305 - accuracy: 0.3525\n",
      "Epoch 00057: loss improved from 3.43449 to 3.43116, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.4312 - accuracy: 0.3524\n",
      "Epoch 58/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.4290 - accuracy: 0.3524\n",
      "Epoch 00058: loss improved from 3.43116 to 3.42883, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.4288 - accuracy: 0.3524\n",
      "Epoch 59/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.4264 - accuracy: 0.3531\n",
      "Epoch 00059: loss improved from 3.42883 to 3.42632, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.4263 - accuracy: 0.3532\n",
      "Epoch 60/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.4228 - accuracy: 0.3536\n",
      "Epoch 00060: loss improved from 3.42632 to 3.42304, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.4230 - accuracy: 0.3536\n",
      "Epoch 61/500\n",
      "224768/225490 [============================>.] - ETA: 0s - loss: 3.4192 - accuracy: 0.3541\n",
      "Epoch 00061: loss improved from 3.42304 to 3.41880, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.4188 - accuracy: 0.3542\n",
      "Epoch 62/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.4160 - accuracy: 0.3549\n",
      "Epoch 00062: loss improved from 3.41880 to 3.41623, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.4162 - accuracy: 0.3549\n",
      "Epoch 63/500\n",
      "224768/225490 [============================>.] - ETA: 0s - loss: 3.4134 - accuracy: 0.3550\n",
      "Epoch 00063: loss improved from 3.41623 to 3.41351, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.4135 - accuracy: 0.3550\n",
      "Epoch 64/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.4133 - accuracy: 0.3556\n",
      "Epoch 00064: loss improved from 3.41351 to 3.41337, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.4134 - accuracy: 0.3556\n",
      "Epoch 65/500\n",
      "224896/225490 [============================>.] - ETA: 0s - loss: 3.4074 - accuracy: 0.3567\n",
      "Epoch 00065: loss improved from 3.41337 to 3.40717, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.4072 - accuracy: 0.3567\n",
      "Epoch 66/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.4037 - accuracy: 0.3569\n",
      "Epoch 00066: loss improved from 3.40717 to 3.40382, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.4038 - accuracy: 0.3568\n",
      "Epoch 67/500\n",
      "224896/225490 [============================>.] - ETA: 0s - loss: 3.4014 - accuracy: 0.3571\n",
      "Epoch 00067: loss improved from 3.40382 to 3.40140, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 80us/sample - loss: 3.4014 - accuracy: 0.3571\n",
      "Epoch 68/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.3996 - accuracy: 0.3574\n",
      "Epoch 00068: loss improved from 3.40140 to 3.39946, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.3995 - accuracy: 0.3574\n",
      "Epoch 69/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.3973 - accuracy: 0.3580\n",
      "Epoch 00069: loss improved from 3.39946 to 3.39722, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.3972 - accuracy: 0.3580\n",
      "Epoch 70/500\n",
      "224768/225490 [============================>.] - ETA: 0s - loss: 3.3935 - accuracy: 0.3579\n",
      "Epoch 00070: loss improved from 3.39722 to 3.39316, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.3932 - accuracy: 0.3579\n",
      "Epoch 71/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.3908 - accuracy: 0.3591\n",
      "Epoch 00071: loss improved from 3.39316 to 3.39090, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.3909 - accuracy: 0.3591\n",
      "Epoch 72/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.3888 - accuracy: 0.3587\n",
      "Epoch 00072: loss improved from 3.39090 to 3.38871, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.3887 - accuracy: 0.3587\n",
      "Epoch 73/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.3855 - accuracy: 0.3593\n",
      "Epoch 00073: loss improved from 3.38871 to 3.38554, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 78us/sample - loss: 3.3855 - accuracy: 0.3593\n",
      "Epoch 74/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.3819 - accuracy: 0.3605\n",
      "Epoch 00074: loss improved from 3.38554 to 3.38199, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.3820 - accuracy: 0.3606\n",
      "Epoch 75/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.3801 - accuracy: 0.3606\n",
      "Epoch 00075: loss improved from 3.38199 to 3.38019, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.3802 - accuracy: 0.3606\n",
      "Epoch 76/500\n",
      "224768/225490 [============================>.] - ETA: 0s - loss: 3.3771 - accuracy: 0.3611\n",
      "Epoch 00076: loss improved from 3.38019 to 3.37804, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.3780 - accuracy: 0.3610\n",
      "Epoch 77/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.3755 - accuracy: 0.3608\n",
      "Epoch 00077: loss improved from 3.37804 to 3.37545, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.3754 - accuracy: 0.3608\n",
      "Epoch 78/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.3741 - accuracy: 0.3612\n",
      "Epoch 00078: loss improved from 3.37545 to 3.37395, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.3739 - accuracy: 0.3613\n",
      "Epoch 79/500\n",
      "224896/225490 [============================>.] - ETA: 0s - loss: 3.3700 - accuracy: 0.3614\n",
      "Epoch 00079: loss improved from 3.37395 to 3.36972, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.3697 - accuracy: 0.3615\n",
      "Epoch 80/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.3665 - accuracy: 0.3621\n",
      "Epoch 00080: loss improved from 3.36972 to 3.36641, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.3664 - accuracy: 0.3620\n",
      "Epoch 81/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.3650 - accuracy: 0.3630\n",
      "Epoch 00081: loss improved from 3.36641 to 3.36506, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.3651 - accuracy: 0.3630\n",
      "Epoch 82/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.3632 - accuracy: 0.3627\n",
      "Epoch 00082: loss improved from 3.36506 to 3.36298, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.3630 - accuracy: 0.3627\n",
      "Epoch 83/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.3600 - accuracy: 0.3634\n",
      "Epoch 00083: loss improved from 3.36298 to 3.36022, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.3602 - accuracy: 0.3634\n",
      "Epoch 84/500\n",
      "224896/225490 [============================>.] - ETA: 0s - loss: 3.3570 - accuracy: 0.3637\n",
      "Epoch 00084: loss improved from 3.36022 to 3.35734, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.3573 - accuracy: 0.3637\n",
      "Epoch 85/500\n",
      "224768/225490 [============================>.] - ETA: 0s - loss: 3.3552 - accuracy: 0.3641\n",
      "Epoch 00085: loss improved from 3.35734 to 3.35497, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.3550 - accuracy: 0.3642\n",
      "Epoch 86/500\n",
      "224896/225490 [============================>.] - ETA: 0s - loss: 3.3524 - accuracy: 0.3651\n",
      "Epoch 00086: loss improved from 3.35497 to 3.35245, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.3525 - accuracy: 0.3650\n",
      "Epoch 87/500\n",
      "224896/225490 [============================>.] - ETA: 0s - loss: 3.3500 - accuracy: 0.3647\n",
      "Epoch 00087: loss improved from 3.35245 to 3.35018, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.3502 - accuracy: 0.3647\n",
      "Epoch 88/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.3474 - accuracy: 0.3655\n",
      "Epoch 00088: loss improved from 3.35018 to 3.34741, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.3474 - accuracy: 0.3655\n",
      "Epoch 89/500\n",
      "224768/225490 [============================>.] - ETA: 0s - loss: 3.3458 - accuracy: 0.3657\n",
      "Epoch 00089: loss improved from 3.34741 to 3.34619, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.3462 - accuracy: 0.3656\n",
      "Epoch 90/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.3454 - accuracy: 0.3663\n",
      "Epoch 00090: loss improved from 3.34619 to 3.34502, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.3450 - accuracy: 0.3663\n",
      "Epoch 91/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.3419 - accuracy: 0.3663\n",
      "Epoch 00091: loss improved from 3.34502 to 3.34197, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.3420 - accuracy: 0.3663\n",
      "Epoch 92/500\n",
      "224768/225490 [============================>.] - ETA: 0s - loss: 3.3380 - accuracy: 0.3669\n",
      "Epoch 00092: loss improved from 3.34197 to 3.33797, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.3380 - accuracy: 0.3669\n",
      "Epoch 93/500\n",
      "224896/225490 [============================>.] - ETA: 0s - loss: 3.3375 - accuracy: 0.3660\n",
      "Epoch 00093: loss improved from 3.33797 to 3.33725, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 22s 97us/sample - loss: 3.3372 - accuracy: 0.3660\n",
      "Epoch 94/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.3339 - accuracy: 0.3671\n",
      "Epoch 00094: loss improved from 3.33725 to 3.33398, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 21s 95us/sample - loss: 3.3340 - accuracy: 0.3671\n",
      "Epoch 95/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.3338 - accuracy: 0.3674\n",
      "Epoch 00095: loss improved from 3.33398 to 3.33330, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 81us/sample - loss: 3.3333 - accuracy: 0.3675\n",
      "Epoch 96/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.3310 - accuracy: 0.3680\n",
      "Epoch 00096: loss improved from 3.33330 to 3.33140, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 80us/sample - loss: 3.3314 - accuracy: 0.3680\n",
      "Epoch 97/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.3294 - accuracy: 0.3687\n",
      "Epoch 00097: loss improved from 3.33140 to 3.32953, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 81us/sample - loss: 3.3295 - accuracy: 0.3687\n",
      "Epoch 98/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.3268 - accuracy: 0.3689\n",
      "Epoch 00098: loss improved from 3.32953 to 3.32686, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 80us/sample - loss: 3.3269 - accuracy: 0.3689\n",
      "Epoch 99/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.3249 - accuracy: 0.3687\n",
      "Epoch 00099: loss improved from 3.32686 to 3.32513, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 81us/sample - loss: 3.3251 - accuracy: 0.3687\n",
      "Epoch 100/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.3216 - accuracy: 0.3695\n",
      "Epoch 00100: loss improved from 3.32513 to 3.32145, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 82us/sample - loss: 3.3214 - accuracy: 0.3696\n",
      "Epoch 101/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.3212 - accuracy: 0.3695\n",
      "Epoch 00101: loss improved from 3.32145 to 3.32116, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 81us/sample - loss: 3.3212 - accuracy: 0.3695\n",
      "Epoch 102/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.3170 - accuracy: 0.3706\n",
      "Epoch 00102: loss improved from 3.32116 to 3.31716, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 81us/sample - loss: 3.3172 - accuracy: 0.3706\n",
      "Epoch 103/500\n",
      "224768/225490 [============================>.] - ETA: 0s - loss: 3.3168 - accuracy: 0.3698\n",
      "Epoch 00103: loss improved from 3.31716 to 3.31675, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 80us/sample - loss: 3.3168 - accuracy: 0.3698\n",
      "Epoch 104/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.3146 - accuracy: 0.3703\n",
      "Epoch 00104: loss improved from 3.31675 to 3.31469, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 81us/sample - loss: 3.3147 - accuracy: 0.3703\n",
      "Epoch 105/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.3123 - accuracy: 0.3710\n",
      "Epoch 00105: loss improved from 3.31469 to 3.31235, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 81us/sample - loss: 3.3123 - accuracy: 0.3710\n",
      "Epoch 106/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.3104 - accuracy: 0.3715\n",
      "Epoch 00106: loss improved from 3.31235 to 3.31074, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 81us/sample - loss: 3.3107 - accuracy: 0.3714\n",
      "Epoch 107/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.3080 - accuracy: 0.3715\n",
      "Epoch 00107: loss improved from 3.31074 to 3.30832, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 19s 82us/sample - loss: 3.3083 - accuracy: 0.3715\n",
      "Epoch 108/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.3061 - accuracy: 0.3723\n",
      "Epoch 00108: loss improved from 3.30832 to 3.30587, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 19s 84us/sample - loss: 3.3059 - accuracy: 0.3723\n",
      "Epoch 109/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.3049 - accuracy: 0.3717\n",
      "Epoch 00109: loss improved from 3.30587 to 3.30498, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.3050 - accuracy: 0.3717\n",
      "Epoch 110/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.3029 - accuracy: 0.3721\n",
      "Epoch 00110: loss improved from 3.30498 to 3.30321, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.3032 - accuracy: 0.3721\n",
      "Epoch 111/500\n",
      "224896/225490 [============================>.] - ETA: 0s - loss: 3.3003 - accuracy: 0.3724\n",
      "Epoch 00111: loss improved from 3.30321 to 3.30025, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 81us/sample - loss: 3.3003 - accuracy: 0.3725\n",
      "Epoch 112/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.2987 - accuracy: 0.3730\n",
      "Epoch 00112: loss improved from 3.30025 to 3.29905, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.2991 - accuracy: 0.3729\n",
      "Epoch 113/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.2978 - accuracy: 0.3737\n",
      "Epoch 00113: loss improved from 3.29905 to 3.29785, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.2978 - accuracy: 0.3737\n",
      "Epoch 114/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.2950 - accuracy: 0.3732\n",
      "Epoch 00114: loss improved from 3.29785 to 3.29471, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 81us/sample - loss: 3.2947 - accuracy: 0.3733\n",
      "Epoch 115/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.2915 - accuracy: 0.3742\n",
      "Epoch 00115: loss improved from 3.29471 to 3.29188, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.2919 - accuracy: 0.3741\n",
      "Epoch 116/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.2912 - accuracy: 0.3732\n",
      "Epoch 00116: loss improved from 3.29188 to 3.29095, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.2910 - accuracy: 0.3732\n",
      "Epoch 117/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.2908 - accuracy: 0.3742\n",
      "Epoch 00117: loss improved from 3.29095 to 3.29083, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 19s 83us/sample - loss: 3.2908 - accuracy: 0.3742\n",
      "Epoch 118/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.2869 - accuracy: 0.3754\n",
      "Epoch 00118: loss improved from 3.29083 to 3.28664, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 81us/sample - loss: 3.2866 - accuracy: 0.3754\n",
      "Epoch 119/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.2876 - accuracy: 0.3746\n",
      "Epoch 00119: loss did not improve from 3.28664\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.2875 - accuracy: 0.3746\n",
      "Epoch 120/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.2832 - accuracy: 0.3750\n",
      "Epoch 00120: loss improved from 3.28664 to 3.28323, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 80us/sample - loss: 3.2832 - accuracy: 0.3750\n",
      "Epoch 121/500\n",
      "224768/225490 [============================>.] - ETA: 0s - loss: 3.2836 - accuracy: 0.3760\n",
      "Epoch 00121: loss did not improve from 3.28323\n",
      "225490/225490 [==============================] - 18s 80us/sample - loss: 3.2841 - accuracy: 0.3759\n",
      "Epoch 122/500\n",
      "224896/225490 [============================>.] - ETA: 0s - loss: 3.2806 - accuracy: 0.3757\n",
      "Epoch 00122: loss improved from 3.28323 to 3.28089, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 80us/sample - loss: 3.2809 - accuracy: 0.3757\n",
      "Epoch 123/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.2788 - accuracy: 0.3756\n",
      "Epoch 00123: loss improved from 3.28089 to 3.27888, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.2789 - accuracy: 0.3756\n",
      "Epoch 124/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.2777 - accuracy: 0.3767\n",
      "Epoch 00124: loss improved from 3.27888 to 3.27756, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.2776 - accuracy: 0.3767\n",
      "Epoch 125/500\n",
      "224896/225490 [============================>.] - ETA: 0s - loss: 3.2759 - accuracy: 0.3765\n",
      "Epoch 00125: loss improved from 3.27756 to 3.27605, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 19s 82us/sample - loss: 3.2760 - accuracy: 0.3765\n",
      "Epoch 126/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.2752 - accuracy: 0.3765\n",
      "Epoch 00126: loss improved from 3.27605 to 3.27522, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 80us/sample - loss: 3.2752 - accuracy: 0.3764\n",
      "Epoch 127/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.2729 - accuracy: 0.3775\n",
      "Epoch 00127: loss improved from 3.27522 to 3.27277, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 80us/sample - loss: 3.2728 - accuracy: 0.3775\n",
      "Epoch 128/500\n",
      "224896/225490 [============================>.] - ETA: 0s - loss: 3.2701 - accuracy: 0.3776\n",
      "Epoch 00128: loss improved from 3.27277 to 3.27008, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 19s 86us/sample - loss: 3.2701 - accuracy: 0.3775\n",
      "Epoch 129/500\n",
      "224768/225490 [============================>.] - ETA: 0s - loss: 3.2678 - accuracy: 0.3773\n",
      "Epoch 00129: loss improved from 3.27008 to 3.26837, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.2684 - accuracy: 0.3772\n",
      "Epoch 130/500\n",
      "224768/225490 [============================>.] - ETA: 0s - loss: 3.2668 - accuracy: 0.3777\n",
      "Epoch 00130: loss improved from 3.26837 to 3.26672, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 78us/sample - loss: 3.2667 - accuracy: 0.3777\n",
      "Epoch 131/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.2647 - accuracy: 0.3775\n",
      "Epoch 00131: loss improved from 3.26672 to 3.26501, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.2650 - accuracy: 0.3775\n",
      "Epoch 132/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.2668 - accuracy: 0.3778\n",
      "Epoch 00132: loss did not improve from 3.26501\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.2668 - accuracy: 0.3778\n",
      "Epoch 133/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.2635 - accuracy: 0.3775\n",
      "Epoch 00133: loss improved from 3.26501 to 3.26355, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.2636 - accuracy: 0.3775\n",
      "Epoch 134/500\n",
      "224768/225490 [============================>.] - ETA: 0s - loss: 3.2597 - accuracy: 0.3785\n",
      "Epoch 00134: loss improved from 3.26355 to 3.25939, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.2594 - accuracy: 0.3786\n",
      "Epoch 135/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.2598 - accuracy: 0.3779\n",
      "Epoch 00135: loss did not improve from 3.25939\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.2597 - accuracy: 0.3779\n",
      "Epoch 136/500\n",
      "224768/225490 [============================>.] - ETA: 0s - loss: 3.2571 - accuracy: 0.3786\n",
      "Epoch 00136: loss improved from 3.25939 to 3.25715, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.2572 - accuracy: 0.3786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.2573 - accuracy: 0.3784\n",
      "Epoch 00137: loss did not improve from 3.25715\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.2572 - accuracy: 0.3784\n",
      "Epoch 138/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.2546 - accuracy: 0.3800\n",
      "Epoch 00138: loss improved from 3.25715 to 3.25485, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 78us/sample - loss: 3.2548 - accuracy: 0.3800\n",
      "Epoch 139/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.2543 - accuracy: 0.3796\n",
      "Epoch 00139: loss improved from 3.25485 to 3.25445, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.2544 - accuracy: 0.3796\n",
      "Epoch 140/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.2511 - accuracy: 0.3804\n",
      "Epoch 00140: loss improved from 3.25445 to 3.25145, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.2514 - accuracy: 0.3803\n",
      "Epoch 141/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.2514 - accuracy: 0.3795\n",
      "Epoch 00141: loss did not improve from 3.25145\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.2515 - accuracy: 0.3795\n",
      "Epoch 142/500\n",
      "224896/225490 [============================>.] - ETA: 0s - loss: 3.2474 - accuracy: 0.3804\n",
      "Epoch 00142: loss improved from 3.25145 to 3.24758, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.2476 - accuracy: 0.3804\n",
      "Epoch 143/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.2471 - accuracy: 0.3805\n",
      "Epoch 00143: loss improved from 3.24758 to 3.24716, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.2472 - accuracy: 0.3805\n",
      "Epoch 144/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.2454 - accuracy: 0.3809\n",
      "Epoch 00144: loss improved from 3.24716 to 3.24539, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.2454 - accuracy: 0.3809\n",
      "Epoch 145/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.2429 - accuracy: 0.3811\n",
      "Epoch 00145: loss improved from 3.24539 to 3.24368, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.2437 - accuracy: 0.3810\n",
      "Epoch 146/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.2435 - accuracy: 0.3805\n",
      "Epoch 00146: loss improved from 3.24368 to 3.24339, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.2434 - accuracy: 0.3805\n",
      "Epoch 147/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.2424 - accuracy: 0.3811\n",
      "Epoch 00147: loss improved from 3.24339 to 3.24257, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.2426 - accuracy: 0.3811\n",
      "Epoch 148/500\n",
      "224768/225490 [============================>.] - ETA: 0s - loss: 3.2399 - accuracy: 0.3820\n",
      "Epoch 00148: loss improved from 3.24257 to 3.23982, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.2398 - accuracy: 0.3820\n",
      "Epoch 149/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.2409 - accuracy: 0.3817\n",
      "Epoch 00149: loss did not improve from 3.23982\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.2407 - accuracy: 0.3817\n",
      "Epoch 150/500\n",
      "224896/225490 [============================>.] - ETA: 0s - loss: 3.2397 - accuracy: 0.3820\n",
      "Epoch 00150: loss improved from 3.23982 to 3.23961, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.2396 - accuracy: 0.3820\n",
      "Epoch 151/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.2358 - accuracy: 0.3825\n",
      "Epoch 00151: loss improved from 3.23961 to 3.23558, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.2356 - accuracy: 0.3826\n",
      "Epoch 152/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.2363 - accuracy: 0.3822\n",
      "Epoch 00152: loss did not improve from 3.23558\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.2365 - accuracy: 0.3821\n",
      "Epoch 153/500\n",
      "224896/225490 [============================>.] - ETA: 0s - loss: 3.2346 - accuracy: 0.3826\n",
      "Epoch 00153: loss improved from 3.23558 to 3.23450, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.2345 - accuracy: 0.3826\n",
      "Epoch 154/500\n",
      "224896/225490 [============================>.] - ETA: 0s - loss: 3.2326 - accuracy: 0.3826\n",
      "Epoch 00154: loss improved from 3.23450 to 3.23306, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.2331 - accuracy: 0.3825\n",
      "Epoch 155/500\n",
      "224768/225490 [============================>.] - ETA: 0s - loss: 3.2303 - accuracy: 0.3835\n",
      "Epoch 00155: loss improved from 3.23306 to 3.23054, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.2305 - accuracy: 0.3835\n",
      "Epoch 156/500\n",
      "224768/225490 [============================>.] - ETA: 0s - loss: 3.2301 - accuracy: 0.3825\n",
      "Epoch 00156: loss improved from 3.23054 to 3.23039, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.2304 - accuracy: 0.3825\n",
      "Epoch 157/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.2284 - accuracy: 0.3828\n",
      "Epoch 00157: loss improved from 3.23039 to 3.22831, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.2283 - accuracy: 0.3828\n",
      "Epoch 158/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.2277 - accuracy: 0.3839\n",
      "Epoch 00158: loss improved from 3.22831 to 3.22780, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.2278 - accuracy: 0.3839\n",
      "Epoch 159/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.2261 - accuracy: 0.3840 ETA: 0s - loss: 3.2259 - accuracy: 0.\n",
      "Epoch 00159: loss improved from 3.22780 to 3.22599, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.2260 - accuracy: 0.3839\n",
      "Epoch 160/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.2248 - accuracy: 0.3840\n",
      "Epoch 00160: loss improved from 3.22599 to 3.22464, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.2246 - accuracy: 0.3841\n",
      "Epoch 161/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.2258 - accuracy: 0.3833\n",
      "Epoch 00161: loss did not improve from 3.22464\n",
      "225490/225490 [==============================] - 18s 80us/sample - loss: 3.2260 - accuracy: 0.3832\n",
      "Epoch 162/500\n",
      "224896/225490 [============================>.] - ETA: 0s - loss: 3.2203 - accuracy: 0.3846\n",
      "Epoch 00162: loss improved from 3.22464 to 3.22080, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.2208 - accuracy: 0.3844\n",
      "Epoch 163/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.2207 - accuracy: 0.38 - ETA: 0s - loss: 3.2205 - accuracy: 0.3844\n",
      "Epoch 00163: loss improved from 3.22080 to 3.22076, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.2208 - accuracy: 0.3844\n",
      "Epoch 164/500\n",
      "224896/225490 [============================>.] - ETA: 0s - loss: 3.2187 - accuracy: 0.3847\n",
      "Epoch 00164: loss improved from 3.22076 to 3.21909, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.2191 - accuracy: 0.3847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.2200 - accuracy: 0.3843\n",
      "Epoch 00165: loss did not improve from 3.21909\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.2197 - accuracy: 0.3843\n",
      "Epoch 166/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.2177 - accuracy: 0.3852\n",
      "Epoch 00166: loss improved from 3.21909 to 3.21796, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.2180 - accuracy: 0.3852\n",
      "Epoch 167/500\n",
      "224768/225490 [============================>.] - ETA: 0s - loss: 3.2151 - accuracy: 0.3856 ETA\n",
      "Epoch 00167: loss improved from 3.21796 to 3.21527, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.2153 - accuracy: 0.3856\n",
      "Epoch 168/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.2133 - accuracy: 0.3856\n",
      "Epoch 00168: loss improved from 3.21527 to 3.21328, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 81us/sample - loss: 3.2133 - accuracy: 0.3856\n",
      "Epoch 169/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.2131 - accuracy: 0.3860\n",
      "Epoch 00169: loss improved from 3.21328 to 3.21307, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 78us/sample - loss: 3.2131 - accuracy: 0.3860\n",
      "Epoch 170/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.2119 - accuracy: 0.3864\n",
      "Epoch 00170: loss improved from 3.21307 to 3.21238, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.2124 - accuracy: 0.3864\n",
      "Epoch 171/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.2115 - accuracy: 0.3859\n",
      "Epoch 00171: loss improved from 3.21238 to 3.21145, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.2115 - accuracy: 0.3859\n",
      "Epoch 172/500\n",
      "224896/225490 [============================>.] - ETA: 0s - loss: 3.2100 - accuracy: 0.3863\n",
      "Epoch 00172: loss improved from 3.21145 to 3.21016, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.2102 - accuracy: 0.3862\n",
      "Epoch 173/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.2090 - accuracy: 0.3861\n",
      "Epoch 00173: loss improved from 3.21016 to 3.20918, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.2092 - accuracy: 0.3861\n",
      "Epoch 174/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.2073 - accuracy: 0.3864\n",
      "Epoch 00174: loss improved from 3.20918 to 3.20728, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.2073 - accuracy: 0.3864\n",
      "Epoch 175/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.2053 - accuracy: 0.3876\n",
      "Epoch 00175: loss improved from 3.20728 to 3.20534, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.2053 - accuracy: 0.3876\n",
      "Epoch 176/500\n",
      "224896/225490 [============================>.] - ETA: 0s - loss: 3.2069 - accuracy: 0.3869\n",
      "Epoch 00176: loss did not improve from 3.20534\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.2064 - accuracy: 0.3870\n",
      "Epoch 177/500\n",
      "224896/225490 [============================>.] - ETA: 0s - loss: 3.2049 - accuracy: 0.3863\n",
      "Epoch 00177: loss improved from 3.20534 to 3.20483, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.2048 - accuracy: 0.3863\n",
      "Epoch 178/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.2024 - accuracy: 0.3876\n",
      "Epoch 00178: loss improved from 3.20483 to 3.20252, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 78us/sample - loss: 3.2025 - accuracy: 0.3876\n",
      "Epoch 179/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.2024 - accuracy: 0.3877\n",
      "Epoch 00179: loss improved from 3.20252 to 3.20226, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.2023 - accuracy: 0.3877\n",
      "Epoch 180/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.2004 - accuracy: 0.3873 ETA\n",
      "Epoch 00180: loss improved from 3.20226 to 3.20032, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.2003 - accuracy: 0.3873\n",
      "Epoch 181/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.1989 - accuracy: 0.3884\n",
      "Epoch 00181: loss improved from 3.20032 to 3.19903, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.1990 - accuracy: 0.3884\n",
      "Epoch 182/500\n",
      "224768/225490 [============================>.] - ETA: 0s - loss: 3.1989 - accuracy: 0.3879\n",
      "Epoch 00182: loss did not improve from 3.19903\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.1995 - accuracy: 0.3878\n",
      "Epoch 183/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.1964 - accuracy: 0.3875\n",
      "Epoch 00183: loss improved from 3.19903 to 3.19675, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 78us/sample - loss: 3.1968 - accuracy: 0.3875\n",
      "Epoch 184/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.1952 - accuracy: 0.3876\n",
      "Epoch 00184: loss improved from 3.19675 to 3.19530, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.1953 - accuracy: 0.3876\n",
      "Epoch 185/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.1945 - accuracy: 0.3889\n",
      "Epoch 00185: loss improved from 3.19530 to 3.19467, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.1947 - accuracy: 0.3890\n",
      "Epoch 186/500\n",
      "224896/225490 [============================>.] - ETA: 0s - loss: 3.1961 - accuracy: 0.3880\n",
      "Epoch 00186: loss did not improve from 3.19467\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.1967 - accuracy: 0.3880\n",
      "Epoch 187/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.1918 - accuracy: 0.3889\n",
      "Epoch 00187: loss improved from 3.19467 to 3.19182, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.1918 - accuracy: 0.3889\n",
      "Epoch 188/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.1937 - accuracy: 0.3884\n",
      "Epoch 00188: loss did not improve from 3.19182\n",
      "225490/225490 [==============================] - 18s 81us/sample - loss: 3.1936 - accuracy: 0.3885\n",
      "Epoch 189/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.1897 - accuracy: 0.3886\n",
      "Epoch 00189: loss improved from 3.19182 to 3.18976, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.1898 - accuracy: 0.3886\n",
      "Epoch 190/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.1910 - accuracy: 0.3890\n",
      "Epoch 00190: loss did not improve from 3.18976\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.1908 - accuracy: 0.3890\n",
      "Epoch 191/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.1878 - accuracy: 0.3892\n",
      "Epoch 00191: loss improved from 3.18976 to 3.18781, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.1878 - accuracy: 0.3892\n",
      "Epoch 192/500\n",
      "224768/225490 [============================>.] - ETA: 0s - loss: 3.1899 - accuracy: 0.3888\n",
      "Epoch 00192: loss did not improve from 3.18781\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.1904 - accuracy: 0.3887\n",
      "Epoch 193/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.1846 - accuracy: 0.3898\n",
      "Epoch 00193: loss improved from 3.18781 to 3.18484, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.1848 - accuracy: 0.3897\n",
      "Epoch 194/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.1858 - accuracy: 0.3901\n",
      "Epoch 00194: loss did not improve from 3.18484\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.1860 - accuracy: 0.3900\n",
      "Epoch 195/500\n",
      "224768/225490 [============================>.] - ETA: 0s - loss: 3.1847 - accuracy: 0.3896\n",
      "Epoch 00195: loss improved from 3.18484 to 3.18447, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.1845 - accuracy: 0.3897\n",
      "Epoch 196/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.1827 - accuracy: 0.3901\n",
      "Epoch 00196: loss improved from 3.18447 to 3.18278, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.1828 - accuracy: 0.3901\n",
      "Epoch 197/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.1828 - accuracy: 0.3896\n",
      "Epoch 00197: loss improved from 3.18278 to 3.18261, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.1826 - accuracy: 0.3896\n",
      "Epoch 198/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.1815 - accuracy: 0.3910\n",
      "Epoch 00198: loss improved from 3.18261 to 3.18141, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.1814 - accuracy: 0.3911\n",
      "Epoch 199/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.1832 - accuracy: 0.3889\n",
      "Epoch 00199: loss did not improve from 3.18141\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.1833 - accuracy: 0.3889\n",
      "Epoch 200/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.1787 - accuracy: 0.3907 ETA: 0s - loss:\n",
      "Epoch 00200: loss improved from 3.18141 to 3.17899, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.1790 - accuracy: 0.3907\n",
      "Epoch 201/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.1779 - accuracy: 0.3915\n",
      "Epoch 00201: loss improved from 3.17899 to 3.17789, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.1779 - accuracy: 0.3915\n",
      "Epoch 202/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.1768 - accuracy: 0.3907\n",
      "Epoch 00202: loss improved from 3.17789 to 3.17702, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.1770 - accuracy: 0.3907\n",
      "Epoch 203/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.1762 - accuracy: 0.3913\n",
      "Epoch 00203: loss improved from 3.17702 to 3.17631, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.1763 - accuracy: 0.3913\n",
      "Epoch 204/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.1766 - accuracy: 0.3913\n",
      "Epoch 00204: loss did not improve from 3.17631\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.1766 - accuracy: 0.3913\n",
      "Epoch 205/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.1736 - accuracy: 0.3926\n",
      "Epoch 00205: loss improved from 3.17631 to 3.17360, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.1736 - accuracy: 0.3926\n",
      "Epoch 206/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.1734 - accuracy: 0.3914\n",
      "Epoch 00206: loss improved from 3.17360 to 3.17333, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 78us/sample - loss: 3.1733 - accuracy: 0.3914\n",
      "Epoch 207/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.1737 - accuracy: 0.3912\n",
      "Epoch 00207: loss did not improve from 3.17333\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.1741 - accuracy: 0.3912\n",
      "Epoch 208/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.1695 - accuracy: 0.3925\n",
      "Epoch 00208: loss improved from 3.17333 to 3.16968, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.1697 - accuracy: 0.3925\n",
      "Epoch 209/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.1722 - accuracy: 0.3919\n",
      "Epoch 00209: loss did not improve from 3.16968\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.1724 - accuracy: 0.3919\n",
      "Epoch 210/500\n",
      "224768/225490 [============================>.] - ETA: 0s - loss: 3.1683 - accuracy: 0.3926\n",
      "Epoch 00210: loss improved from 3.16968 to 3.16864, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.1686 - accuracy: 0.3925\n",
      "Epoch 211/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.1700 - accuracy: 0.3925\n",
      "Epoch 00211: loss did not improve from 3.16864\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.1699 - accuracy: 0.3925\n",
      "Epoch 212/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.1661 - accuracy: 0.3927\n",
      "Epoch 00212: loss improved from 3.16864 to 3.16642, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 23s 102us/sample - loss: 3.1664 - accuracy: 0.3926\n",
      "Epoch 213/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.1664 - accuracy: 0.3925\n",
      "Epoch 00213: loss improved from 3.16642 to 3.16631, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 19s 82us/sample - loss: 3.1663 - accuracy: 0.3925\n",
      "Epoch 214/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.1651 - accuracy: 0.3925\n",
      "Epoch 00214: loss improved from 3.16631 to 3.16539, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 19s 86us/sample - loss: 3.1654 - accuracy: 0.3924\n",
      "Epoch 215/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.1652 - accuracy: 0.3924\n",
      "Epoch 00215: loss improved from 3.16539 to 3.16525, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 81us/sample - loss: 3.1653 - accuracy: 0.3924\n",
      "Epoch 216/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.1620 - accuracy: 0.3937\n",
      "Epoch 00216: loss improved from 3.16525 to 3.16218, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 80us/sample - loss: 3.1622 - accuracy: 0.3936\n",
      "Epoch 217/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.1649 - accuracy: 0.3929\n",
      "Epoch 00217: loss did not improve from 3.16218\n",
      "225490/225490 [==============================] - 18s 80us/sample - loss: 3.1651 - accuracy: 0.3929\n",
      "Epoch 218/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.1622 - accuracy: 0.3934\n",
      "Epoch 00218: loss did not improve from 3.16218\n",
      "225490/225490 [==============================] - 19s 82us/sample - loss: 3.1622 - accuracy: 0.3934\n",
      "Epoch 219/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.1605 - accuracy: 0.3937\n",
      "Epoch 00219: loss improved from 3.16218 to 3.16071, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 81us/sample - loss: 3.1607 - accuracy: 0.3937\n",
      "Epoch 220/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.1610 - accuracy: 0.3931\n",
      "Epoch 00220: loss did not improve from 3.16071\n",
      "225490/225490 [==============================] - 18s 81us/sample - loss: 3.1610 - accuracy: 0.3931\n",
      "Epoch 221/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.1590 - accuracy: 0.3939\n",
      "Epoch 00221: loss improved from 3.16071 to 3.15935, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 82us/sample - loss: 3.1593 - accuracy: 0.3939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 222/500\n",
      "224896/225490 [============================>.] - ETA: 0s - loss: 3.1583 - accuracy: 0.3935\n",
      "Epoch 00222: loss improved from 3.15935 to 3.15810, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.1581 - accuracy: 0.3936\n",
      "Epoch 223/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.1577 - accuracy: 0.3938\n",
      "Epoch 00223: loss improved from 3.15810 to 3.15751, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.1575 - accuracy: 0.3938\n",
      "Epoch 224/500\n",
      "224896/225490 [============================>.] - ETA: 0s - loss: 3.1586 - accuracy: 0.3932\n",
      "Epoch 00224: loss did not improve from 3.15751\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.1591 - accuracy: 0.3931\n",
      "Epoch 225/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.1556 - accuracy: 0.3937\n",
      "Epoch 00225: loss improved from 3.15751 to 3.15548, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 80us/sample - loss: 3.1555 - accuracy: 0.3937\n",
      "Epoch 226/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.1551 - accuracy: 0.3947\n",
      "Epoch 00226: loss improved from 3.15548 to 3.15500, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.1550 - accuracy: 0.3947\n",
      "Epoch 227/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.1549 - accuracy: 0.3940\n",
      "Epoch 00227: loss improved from 3.15500 to 3.15491, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.1549 - accuracy: 0.3940\n",
      "Epoch 228/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.1552 - accuracy: 0.3940\n",
      "Epoch 00228: loss did not improve from 3.15491\n",
      "225490/225490 [==============================] - 19s 83us/sample - loss: 3.1550 - accuracy: 0.3940\n",
      "Epoch 229/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.1532 - accuracy: 0.3942\n",
      "Epoch 00229: loss improved from 3.15491 to 3.15317, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 81us/sample - loss: 3.1532 - accuracy: 0.3942\n",
      "Epoch 230/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.1515 - accuracy: 0.3953\n",
      "Epoch 00230: loss improved from 3.15317 to 3.15152, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.1515 - accuracy: 0.3953\n",
      "Epoch 231/500\n",
      "224896/225490 [============================>.] - ETA: 0s - loss: 3.1504 - accuracy: 0.3945\n",
      "Epoch 00231: loss improved from 3.15152 to 3.15040, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.1504 - accuracy: 0.3945\n",
      "Epoch 232/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.1507 - accuracy: 0.3944\n",
      "Epoch 00232: loss did not improve from 3.15040\n",
      "225490/225490 [==============================] - 18s 80us/sample - loss: 3.1509 - accuracy: 0.3943\n",
      "Epoch 233/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.1511 - accuracy: 0.3949\n",
      "Epoch 00233: loss did not improve from 3.15040\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.1510 - accuracy: 0.3950\n",
      "Epoch 234/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.1498 - accuracy: 0.3952\n",
      "Epoch 00234: loss improved from 3.15040 to 3.15012, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.1501 - accuracy: 0.3951\n",
      "Epoch 235/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.1478 - accuracy: 0.3953\n",
      "Epoch 00235: loss improved from 3.15012 to 3.14785, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 80us/sample - loss: 3.1478 - accuracy: 0.3953\n",
      "Epoch 236/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.1472 - accuracy: 0.3949\n",
      "Epoch 00236: loss improved from 3.14785 to 3.14724, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 80us/sample - loss: 3.1472 - accuracy: 0.3949\n",
      "Epoch 237/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.1446 - accuracy: 0.3963\n",
      "Epoch 00237: loss improved from 3.14724 to 3.14428, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.1443 - accuracy: 0.3963\n",
      "Epoch 238/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.1481 - accuracy: 0.3949\n",
      "Epoch 00238: loss did not improve from 3.14428\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.1484 - accuracy: 0.3949\n",
      "Epoch 239/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.1436 - accuracy: 0.3963\n",
      "Epoch 00239: loss improved from 3.14428 to 3.14347, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 81us/sample - loss: 3.1435 - accuracy: 0.3964\n",
      "Epoch 240/500\n",
      "224768/225490 [============================>.] - ETA: 0s - loss: 3.1432 - accuracy: 0.3960\n",
      "Epoch 00240: loss did not improve from 3.14347\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.1436 - accuracy: 0.3959\n",
      "Epoch 241/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.1433 - accuracy: 0.3962\n",
      "Epoch 00241: loss improved from 3.14347 to 3.14338, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.1434 - accuracy: 0.3962\n",
      "Epoch 242/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.1423 - accuracy: 0.3958\n",
      "Epoch 00242: loss improved from 3.14338 to 3.14231, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 80us/sample - loss: 3.1423 - accuracy: 0.3958\n",
      "Epoch 243/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.1421 - accuracy: 0.3959\n",
      "Epoch 00243: loss improved from 3.14231 to 3.14219, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 19s 83us/sample - loss: 3.1422 - accuracy: 0.3958\n",
      "Epoch 244/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.1393 - accuracy: 0.3965\n",
      "Epoch 00244: loss improved from 3.14219 to 3.13942, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 79us/sample - loss: 3.1394 - accuracy: 0.3964\n",
      "Epoch 245/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.1411 - accuracy: 0.3960\n",
      "Epoch 00245: loss did not improve from 3.13942\n",
      "225490/225490 [==============================] - 18s 80us/sample - loss: 3.1412 - accuracy: 0.3960\n",
      "Epoch 246/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.1397 - accuracy: 0.3964\n",
      "Epoch 00246: loss did not improve from 3.13942\n",
      "225490/225490 [==============================] - 18s 80us/sample - loss: 3.1398 - accuracy: 0.3964\n",
      "Epoch 247/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.1368 - accuracy: 0.3973\n",
      "Epoch 00247: loss improved from 3.13942 to 3.13699, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 80us/sample - loss: 3.1370 - accuracy: 0.3973\n",
      "Epoch 248/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.1383 - accuracy: 0.3966\n",
      "Epoch 00248: loss did not improve from 3.13699\n",
      "225490/225490 [==============================] - 19s 85us/sample - loss: 3.1381 - accuracy: 0.3967\n",
      "Epoch 249/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.1373 - accuracy: 0.3966\n",
      "Epoch 00249: loss did not improve from 3.13699\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.1374 - accuracy: 0.3965\n",
      "Epoch 250/500\n",
      "224768/225490 [============================>.] - ETA: 0s - loss: 3.1346 - accuracy: 0.3976\n",
      "Epoch 00250: loss improved from 3.13699 to 3.13473, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 78us/sample - loss: 3.1347 - accuracy: 0.3976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 251/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.1383 - accuracy: 0.3961\n",
      "Epoch 00251: loss did not improve from 3.13473\n",
      "225490/225490 [==============================] - 17s 76us/sample - loss: 3.1386 - accuracy: 0.3961\n",
      "Epoch 252/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.1351 - accuracy: 0.3970\n",
      "Epoch 00252: loss did not improve from 3.13473\n",
      "225490/225490 [==============================] - 17s 75us/sample - loss: 3.1354 - accuracy: 0.3969\n",
      "Epoch 253/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.1341 - accuracy: 0.3975\n",
      "Epoch 00253: loss improved from 3.13473 to 3.13427, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.1343 - accuracy: 0.3975\n",
      "Epoch 254/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.1334 - accuracy: 0.3974\n",
      "Epoch 00254: loss improved from 3.13427 to 3.13342, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 76us/sample - loss: 3.1334 - accuracy: 0.3974\n",
      "Epoch 255/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.1342 - accuracy: 0.3968\n",
      "Epoch 00255: loss did not improve from 3.13342\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.1341 - accuracy: 0.3968\n",
      "Epoch 256/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.1327 - accuracy: 0.3975\n",
      "Epoch 00256: loss improved from 3.13342 to 3.13287, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.1329 - accuracy: 0.3975\n",
      "Epoch 257/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.1315 - accuracy: 0.3973\n",
      "Epoch 00257: loss improved from 3.13287 to 3.13173, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 76us/sample - loss: 3.1317 - accuracy: 0.3973\n",
      "Epoch 258/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.1299 - accuracy: 0.3984\n",
      "Epoch 00258: loss improved from 3.13173 to 3.12974, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 75us/sample - loss: 3.1297 - accuracy: 0.3984\n",
      "Epoch 259/500\n",
      "224768/225490 [============================>.] - ETA: 0s - loss: 3.1278 - accuracy: 0.3978\n",
      "Epoch 00259: loss improved from 3.12974 to 3.12815, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.1282 - accuracy: 0.3978\n",
      "Epoch 260/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.1307 - accuracy: 0.3978\n",
      "Epoch 00260: loss did not improve from 3.12815\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.1308 - accuracy: 0.3978\n",
      "Epoch 261/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.1276 - accuracy: 0.3985\n",
      "Epoch 00261: loss improved from 3.12815 to 3.12769, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.1277 - accuracy: 0.3985\n",
      "Epoch 262/500\n",
      "224896/225490 [============================>.] - ETA: 0s - loss: 3.1304 - accuracy: 0.3982\n",
      "Epoch 00262: loss did not improve from 3.12769\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.1299 - accuracy: 0.3983\n",
      "Epoch 263/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.1260 - accuracy: 0.3980\n",
      "Epoch 00263: loss improved from 3.12769 to 3.12607, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 75us/sample - loss: 3.1261 - accuracy: 0.3980\n",
      "Epoch 264/500\n",
      "225024/225490 [============================>.] - ETA: 0s - loss: 3.1258 - accuracy: 0.3980\n",
      "Epoch 00264: loss improved from 3.12607 to 3.12563, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.1256 - accuracy: 0.3980\n",
      "Epoch 265/500\n",
      "224768/225490 [============================>.] - ETA: 0s - loss: 3.1246 - accuracy: 0.3982\n",
      "Epoch 00265: loss improved from 3.12563 to 3.12504, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.1250 - accuracy: 0.3982\n",
      "Epoch 266/500\n",
      "224896/225490 [============================>.] - ETA: 0s - loss: 3.1251 - accuracy: 0.3991\n",
      "Epoch 00266: loss improved from 3.12504 to 3.12462, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.1246 - accuracy: 0.3992\n",
      "Epoch 267/500\n",
      "224896/225490 [============================>.] - ETA: 0s - loss: 3.1260 - accuracy: 0.3982\n",
      "Epoch 00267: loss did not improve from 3.12462\n",
      "225490/225490 [==============================] - 17s 76us/sample - loss: 3.1257 - accuracy: 0.3983\n",
      "Epoch 268/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.1246 - accuracy: 0.3984\n",
      "Epoch 00268: loss did not improve from 3.12462\n",
      "225490/225490 [==============================] - 17s 76us/sample - loss: 3.1248 - accuracy: 0.3983\n",
      "Epoch 269/500\n",
      "224896/225490 [============================>.] - ETA: 0s - loss: 3.1223 - accuracy: 0.3988\n",
      "Epoch 00269: loss improved from 3.12462 to 3.12213, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 18s 78us/sample - loss: 3.1221 - accuracy: 0.3989\n",
      "Epoch 270/500\n",
      "225408/225490 [============================>.] - ETA: 0s - loss: 3.1193 - accuracy: 0.3997\n",
      "Epoch 00270: loss improved from 3.12213 to 3.11941, saving model to models/enron_wordRNN_0714.h5\n",
      "225490/225490 [==============================] - 17s 75us/sample - loss: 3.1194 - accuracy: 0.3997\n",
      "Epoch 271/500\n",
      "224896/225490 [============================>.] - ETA: 0s - loss: 3.1223 - accuracy: 0.3997\n",
      "Epoch 00271: loss did not improve from 3.11941\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.1222 - accuracy: 0.3997\n",
      "Epoch 272/500\n",
      "225280/225490 [============================>.] - ETA: 0s - loss: 3.1217 - accuracy: 0.3986\n",
      "Epoch 00272: loss did not improve from 3.11941\n",
      "225490/225490 [==============================] - 17s 77us/sample - loss: 3.1216 - accuracy: 0.3986\n",
      "Epoch 273/500\n",
      "225152/225490 [============================>.] - ETA: 0s - loss: 3.1198 - accuracy: 0.3994\n",
      "Epoch 00273: loss did not improve from 3.11941\n",
      "225490/225490 [==============================] - 17s 76us/sample - loss: 3.1200 - accuracy: 0.3993\n",
      "Epoch 00273: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x144e4868bc8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "path = 'models/enron_wordRNN_0714.h5'\n",
    "checkpoint = ModelCheckpoint(path, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "stopping = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=3)\n",
    "model.fit(train_inputs,train_targets,batch_size=128,epochs=500,verbose=1,callbacks=[checkpoint, stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(path)\n",
    "\n",
    "from pickle import dump\n",
    "dump(tokenizer,open('models/enron_tokenizer_wordRNN_0714','wb'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction \n",
    "\n",
    "### Large dataset\n",
    "\n",
    "Training results:\n",
    "\n",
    "- Epoch 273/500 loss: 3.1200 - accuracy: 0.3993\n",
    "- Each epoch takes about 80-100 seconds on GPU (RTX 2070 SUPER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from pickle import load\n",
    "\n",
    "model = load_model('models/enron_wordRNN_0714.h5')\n",
    "tokenizer = load(open('models/enron_tokenizer_wordRNN_0714','rb'))\n",
    "seq_len = 3 \n",
    "def gen_text(model, tokenizer, seq_len, seed_text, num_gen_words):\n",
    "    output_text = []\n",
    "    input_text = seed_text\n",
    "    for i in range(num_gen_words):\n",
    "        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len,truncating='pre')\n",
    "        pred_word_ind = model.predict_classes(pad_encoded,verbose=0)[0]\n",
    "        \n",
    "        pred_word = tokenizer.index_word[pred_word_ind]\n",
    "        input_text += ' '+pred_word\n",
    "        output_text.append(pred_word)\n",
    "    return ' '.join(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===>Enter --exit to exit from the program\n",
      "Enter string: discussions\n",
      "Output: discussions as the arcade and the\n",
      "\n",
      "Enter string: meetings\n",
      "Output: meetings as hot for the restructuring\n",
      "\n",
      "Enter string: presentation\n",
      "Output: presentation financials to strong long and\n",
      "\n",
      "Enter string: stock\n",
      "Output: stock to the immediate value ability\n",
      "\n",
      "Enter string: finance\n",
      "Output: finance as trades and resource rental\n",
      "\n",
      "Enter string: --exit\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\n===>Enter --exit to exit from the program')\n",
    "while True:\n",
    "    seed_text  = input('Enter string: ')\n",
    "    if seed_text.lower() == '--exit':\n",
    "        break\n",
    "    else:\n",
    "        out = gen_text(model, tokenizer, seq_len=seq_len, seed_text=seed_text, num_gen_words=5)\n",
    "        print('Output: '+seed_text+' '+out)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===>Enter --exit to exit from the program\n",
      "Enter string: business plan is\n",
      "Output: business plan is the largest term must be\n",
      "\n",
      "Enter string: round table discussion\n",
      "Output: round table discussion of the value of the\n",
      "\n",
      "Enter string: business meetings are\n",
      "Output: business meetings are expected to rise on the\n",
      "\n",
      "Enter string: business meetings\n",
      "Output: business meetings as exodus documents to manage\n",
      "\n",
      "Enter string: business\n",
      "Output: business and potentially mexico and introducing\n",
      "\n",
      "Enter string: --exit\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\n===>Enter --exit to exit from the program')\n",
    "while True:\n",
    "    seed_text  = input('Enter string: ')\n",
    "    if seed_text.lower() == '--exit':\n",
    "        break\n",
    "    else:\n",
    "        out = gen_text(model, tokenizer, seq_len=seq_len, seed_text=seed_text, num_gen_words=5)\n",
    "        print('Output: '+seed_text+' '+out)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smaller dataset\n",
    "\n",
    "Training results:\n",
    "\n",
    "- Epoch 500/500 loss: 1.4459 - accuracy: 0.6336\n",
    "- Each epoch takes about 80-100 seconds on GPU (RTX 2070 SUPER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===>Enter --exit to exit from the program\n",
      "Enter string: meeting\n",
      "Output: meeting meeting multi singapore singapore designer\n",
      "\n",
      "Enter string: discussions\n",
      "Output: discussions have to one last not\n",
      "\n",
      "Enter string: presentation\n",
      "Output: presentation have to one last not\n",
      "\n",
      "Enter string: stock\n",
      "Output: stock have to one last not\n",
      "\n",
      "Enter string: finance\n",
      "Output: finance have to one last not\n",
      "\n",
      "Enter string: --exit\n"
     ]
    }
   ],
   "source": [
    "model = load_model('models/outlook_wordRNN_0713.h5')\n",
    "tokenizer = load(open('models/outlook_tokenizer_wordRNN_0713','rb'))\n",
    "seq_len = 3 \n",
    "\n",
    "print('\\n\\n===>Enter --exit to exit from the program')\n",
    "while True:\n",
    "    seed_text  = input('Enter string: ')\n",
    "    if seed_text.lower() == '--exit':\n",
    "        break\n",
    "    else:\n",
    "        out = gen_text(model, tokenizer, seq_len=seq_len, seed_text=seed_text, num_gen_words=5)\n",
    "        print('Output: '+seed_text+' '+out)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===>Enter --exit to exit from the program\n",
      "Enter string: business plan is\n",
      "Output: business plan is have until be only the\n",
      "\n",
      "Enter string: round table discussion\n",
      "Output: round table discussion have to one last not\n",
      "\n",
      "Enter string: business meetings are\n",
      "Output: business meetings are have data designer protect this\n",
      "\n",
      "Enter string: business meetings\n",
      "Output: business meetings have to one last not\n",
      "\n",
      "Enter string: --exit\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\n===>Enter --exit to exit from the program')\n",
    "while True:\n",
    "    seed_text  = input('Enter string: ')\n",
    "    if seed_text.lower() == '--exit':\n",
    "        break\n",
    "    else:\n",
    "        out = gen_text(model, tokenizer, seq_len=seq_len, seed_text=seed_text, num_gen_words=5)\n",
    "        print('Output: '+seed_text+' '+out)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "- A larger dataset is able to predict the next word better\n",
    "- The predictions are not very accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2",
   "language": "python",
   "name": "tensorflow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}